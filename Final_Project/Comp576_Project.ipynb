{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **1. Preprocess**"
      ],
      "metadata": {
        "id": "V6Jh-RMcWWMn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1.1. Download necessary libraries**"
      ],
      "metadata": {
        "id": "NmhRI3IvYWqO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A2-JPVOXNVpc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e99f87ad-bd48-4b32-ecaa-d16885db7eaa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.12/dist-packages (3.1.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.12/dist-packages (from xgboost) (2.27.5)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install pandas numpy scikit-learn xgboost matplotlib"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1.2. Import libraries and download data**"
      ],
      "metadata": {
        "id": "y7aQWlPvYxf7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "import kagglehub\n",
        "\n",
        "# -------------------------\n",
        "# 1. Download the latest dataset from Kaggle\n",
        "# -------------------------\n",
        "dataset_path = kagglehub.dataset_download(\"arianazmoudeh/airbnbopendata\")\n",
        "print(\"Downloaded dataset at:\", dataset_path)\n",
        "\n",
        "# Automatically find CSV files in the downloaded dataset directory\n",
        "csv_files = [f for f in os.listdir(dataset_path) if f.endswith(\".csv\")]\n",
        "if len(csv_files) == 0:\n",
        "    raise FileNotFoundError(\"No CSV files found in the dataset folder.\")\n",
        "\n",
        "RAW_PATH = os.path.join(dataset_path, csv_files[0])\n",
        "print(\"Using CSV file:\", RAW_PATH)\n",
        "\n",
        "# -------------------------\n",
        "# 2. Output directory\n",
        "# -------------------------\n",
        "OUT_DIR = \"/content/data\"\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "print(\"Output directory:\", OUT_DIR)\n",
        "\n",
        "# -------------------------\n",
        "# 3. Load the dataset\n",
        "# -------------------------\n",
        "df = pd.read_csv(RAW_PATH)\n",
        "print(\"Loaded dataset with shape:\", df.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_emUbLZiWi2t",
        "outputId": "5263e7c4-1478-4830-c3d9-05364431a5ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/arianazmoudeh/airbnbopendata?dataset_version_number=1...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10.5M/10.5M [00:01<00:00, 7.00MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloaded dataset at: /root/.cache/kagglehub/datasets/arianazmoudeh/airbnbopendata/versions/1\n",
            "Using CSV file: /root/.cache/kagglehub/datasets/arianazmoudeh/airbnbopendata/versions/1/Airbnb_Open_Data.csv\n",
            "Output directory: /content/data\n",
            "Loaded dataset with shape: (102599, 26)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-908332901.py:34: DtypeWarning: Columns (25) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df = pd.read_csv(RAW_PATH)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1.3. Preprocessing function**"
      ],
      "metadata": {
        "id": "ZDQVw7IhZI8v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "def preprocess_airbnb(raw_path=RAW_PATH, out_dir=OUT_DIR):\n",
        "    df = pd.read_csv(raw_path)\n",
        "\n",
        "    # Standardize column names: trim spaces, convert to lowercase, replace spaces with underscores\n",
        "    df.columns = [c.strip().lower().replace(\" \", \"_\") for c in df.columns]\n",
        "\n",
        "    # Keep only the columns we need\n",
        "    cols_keep = [\n",
        "        \"price\",\n",
        "        \"service_fee\",\n",
        "        \"room_type\",\n",
        "        \"neighbourhood_group\",\n",
        "        \"neighbourhood\",\n",
        "        \"minimum_nights\",\n",
        "        \"number_of_reviews\",\n",
        "        \"reviews_per_month\",\n",
        "        \"calculated_host_listings_count\",\n",
        "        \"availability_365\",\n",
        "        \"review_rate_number\",\n",
        "        \"lat\",\n",
        "        \"long\",\n",
        "        \"cancellation_policy\",\n",
        "        \"host_identity_verified\",\n",
        "        \"instant_bookable\",\n",
        "        \"construction_year\",\n",
        "        \"country\",\n",
        "        \"country_code\",\n",
        "    ]\n",
        "    cols_keep = [c for c in cols_keep if c in df.columns]\n",
        "    df = df[cols_keep].copy()\n",
        "\n",
        "    target_col = \"price\"\n",
        "\n",
        "    # Clean 'price' and 'service_fee' columns\n",
        "    for col in [target_col, 'service_fee']:\n",
        "        if col in df.columns:\n",
        "            df[col] = df[col].astype(str).str.replace('$', '', regex=False).str.replace(',', '', regex=False)\n",
        "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "\n",
        "    # Basic cleaning: drop rows where price is missing or non-positive\n",
        "    df = df.dropna(subset=[target_col])\n",
        "    df = df[df[target_col] > 0]\n",
        "\n",
        "    numeric_cols = [\n",
        "        \"service_fee\",\n",
        "        \"minimum_nights\",\n",
        "        \"number_of_reviews\",\n",
        "        \"reviews_per_month\",\n",
        "        \"calculated_host_listings_count\",\n",
        "        \"availability_365\",\n",
        "        \"review_rate_number\",\n",
        "        \"lat\",\n",
        "        \"long\",\n",
        "        \"construction_year\",\n",
        "    ]\n",
        "    numeric_cols = [c for c in numeric_cols if c in df.columns]\n",
        "\n",
        "    # Fill NaN values in numeric columns (Median imputation)\n",
        "    for col in numeric_cols:\n",
        "        if df[col].isnull().any():\n",
        "            median_val = df[col].median()\n",
        "            df[col] = df[col].fillna(median_val)\n",
        "            # If median is also NaN (empty column), fill with 0\n",
        "            df[col] = df[col].fillna(0)\n",
        "\n",
        "    categorical_cols = [\n",
        "        \"room_type\",\n",
        "        \"neighbourhood_group\",\n",
        "        \"neighbourhood\",\n",
        "        \"cancellation_policy\",\n",
        "        \"host_identity_verified\",\n",
        "        \"instant_bookable\",\n",
        "        \"country\",\n",
        "        \"country_code\",\n",
        "    ]\n",
        "    categorical_cols = [c for c in categorical_cols if c in df.columns]\n",
        "\n",
        "    # Fill NaN values in categorical columns before encoding\n",
        "    for col in categorical_cols:\n",
        "        if col in df.columns and df[col].isnull().any():\n",
        "            df[col] = df[col].fillna('__MISSING__').astype(str)\n",
        "\n",
        "    # Train/validation/test split\n",
        "    train_df, temp_df = train_test_split(df, test_size=0.2, random_state=42)\n",
        "    val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n",
        "\n",
        "    # Scale numeric features\n",
        "    scaler = StandardScaler()\n",
        "    if numeric_cols:\n",
        "        # Fit on train, transform all\n",
        "        train_df[numeric_cols] = scaler.fit_transform(train_df[numeric_cols])\n",
        "        val_df[numeric_cols] = scaler.transform(val_df[numeric_cols])\n",
        "        test_df[numeric_cols] = scaler.transform(test_df[numeric_cols])\n",
        "\n",
        "    # Ordinal encode categorical features\n",
        "    encoder = OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=-1)\n",
        "\n",
        "    if categorical_cols:\n",
        "        train_cat_encoded = encoder.fit_transform(train_df[categorical_cols])\n",
        "        val_cat_encoded = encoder.transform(val_df[categorical_cols])\n",
        "        test_cat_encoded = encoder.transform(test_df[categorical_cols])\n",
        "\n",
        "        train_df[categorical_cols] = np.where(train_cat_encoded == -1, 0, train_cat_encoded + 1).astype(int)\n",
        "        val_df[categorical_cols] = np.where(val_cat_encoded == -1, 0, val_cat_encoded + 1).astype(int)\n",
        "        test_df[categorical_cols] = np.where(test_cat_encoded == -1, 0, test_cat_encoded + 1).astype(int)\n",
        "\n",
        "    # Save processed datasets\n",
        "    train_path = os.path.join(out_dir, \"train_processed.csv\")\n",
        "    val_path = os.path.join(out_dir, \"val_processed.csv\")\n",
        "    test_path = os.path.join(out_dir, \"test_processed.csv\")\n",
        "\n",
        "    train_df.to_csv(train_path, index=False)\n",
        "    val_df.to_csv(val_path, index=False)\n",
        "    test_df.to_csv(test_path, index=False)\n",
        "\n",
        "    # Save metadata\n",
        "    metadata = {\n",
        "        \"target_col\": target_col,\n",
        "        \"numeric_cols\": numeric_cols,\n",
        "        \"categorical_cols\": categorical_cols,\n",
        "        \"cat_cardinalities\": {},\n",
        "    }\n",
        "\n",
        "    for i, col in enumerate(categorical_cols):\n",
        "        metadata[\"cat_cardinalities\"][col] = len(encoder.categories_[i]) + 1\n",
        "\n",
        "    meta_path = os.path.join(out_dir, \"metadata.json\")\n",
        "    with open(meta_path, \"w\") as f:\n",
        "        json.dump(metadata, f, indent=2)\n",
        "\n",
        "    print(\"Preprocess done.\")\n",
        "    print(f\"Saved: {train_path}\")\n",
        "    print(f\"Saved: {val_path}\")\n",
        "    print(f\"Saved: {test_path}\")\n",
        "    print(f\"Saved: {meta_path}\")"
      ],
      "metadata": {
        "id": "0GP2M9BZX9OO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preprocess_airbnb()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DixB5tOeZggP",
        "outputId": "da8726f3-7fa0-4ea4-e283-29f15baa2fbb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2830127989.py:5: DtypeWarning: Columns (25) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df = pd.read_csv(raw_path)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preprocess done.\n",
            "Saved: /content/data/train_processed.csv\n",
            "Saved: /content/data/val_processed.csv\n",
            "Saved: /content/data/test_processed.csv\n",
            "Saved: /content/data/metadata.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2. Traditional ML Baseline Model**"
      ],
      "metadata": {
        "id": "_Uq8F0f0aHBN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from xgboost import XGBRegressor\n",
        "\n",
        "def load_processed_data(data_dir=OUT_DIR):\n",
        "    with open(os.path.join(data_dir, \"metadata.json\"), \"r\") as f:\n",
        "        meta = json.load(f)\n",
        "\n",
        "    target_col = meta[\"target_col\"]\n",
        "    numeric_cols = meta[\"numeric_cols\"]\n",
        "    categorical_cols = meta[\"categorical_cols\"]\n",
        "\n",
        "    train = pd.read_csv(os.path.join(data_dir, \"train_processed.csv\"))\n",
        "    val   = pd.read_csv(os.path.join(data_dir, \"val_processed.csv\"))\n",
        "    test  = pd.read_csv(os.path.join(data_dir, \"test_processed.csv\"))\n",
        "\n",
        "    feature_cols = numeric_cols + categorical_cols\n",
        "\n",
        "    X_train = train[feature_cols].values\n",
        "    y_train = train[target_col].values\n",
        "\n",
        "    X_val = val[feature_cols].values\n",
        "    y_val = val[target_col].values\n",
        "\n",
        "    X_test = test[feature_cols].values\n",
        "    y_test = test[target_col].values\n",
        "\n",
        "    return (X_train, y_train, X_val, y_val, X_test, y_test, meta)"
      ],
      "metadata": {
        "id": "jwLWyTBkaNxL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, y_train, X_val, y_val, X_test, y_test, meta = load_processed_data()\n",
        "\n",
        "def evaluate_model(name, model, X_train, y_train, X_val, y_val):\n",
        "    model.fit(X_train, y_train)\n",
        "    pred_val = model.predict(X_val)\n",
        "\n",
        "    # Calculate MSE first, then take the square root for RMSE, as 'squared' might not be supported.\n",
        "    mse = mean_squared_error(y_val, pred_val)\n",
        "    rmse = mse**0.5\n",
        "    mae  = mean_absolute_error(y_val, pred_val)\n",
        "\n",
        "    print(f\"{name} -> val RMSE: {rmse:.3f}, MAE: {mae:.3f}\")\n",
        "\n",
        "# Redefine the function to fix the 'squared' argument issue locally\n",
        "def evaluate_baseline_full(name, model, X_train, y_train, X_val, y_val, X_test, y_test):\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # VAL\n",
        "    pred_val = model.predict(X_val)\n",
        "    mse_val = mean_squared_error(y_val, pred_val)\n",
        "    rmse_val = mse_val ** 0.5  # Calculate RMSE manually\n",
        "    mae_val  = mean_absolute_error(y_val, pred_val)\n",
        "\n",
        "    # TEST\n",
        "    pred_test = model.predict(X_test)\n",
        "    mse_test = mean_squared_error(y_test, pred_test)\n",
        "    rmse_test = mse_test ** 0.5  # Calculate RMSE manually\n",
        "    mae_test  = mean_absolute_error(y_test, pred_test)\n",
        "\n",
        "    print(f\"{name} -> VAL  RMSE: {rmse_val:.3f}, MAE: {mae_val:.3f}\")\n",
        "    print(f\"{name} -> TEST RMSE: {rmse_test:.3f}, MAE: {mae_test:.3f}\")"
      ],
      "metadata": {
        "id": "lVhQSiE4aXuq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2.1. Random Forest**"
      ],
      "metadata": {
        "id": "2rN8013waiPC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Random Forest\n",
        "X_train, y_train, X_val, y_val, X_test, y_test, meta = load_processed_data()\n",
        "\n",
        "rf = RandomForestRegressor(\n",
        "    n_estimators=50,\n",
        "    max_depth=5,\n",
        "    min_samples_split=50,\n",
        "    min_samples_leaf=20,\n",
        "    random_state=42,\n",
        "    n_jobs=-1,\n",
        ")\n",
        "evaluate_baseline_full(\"RandomForest\", rf,\n",
        "                       X_train, y_train,\n",
        "                       X_val, y_val,\n",
        "                       X_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mCj3zwBYahuR",
        "outputId": "2e663dab-5f6c-4bac-cc94-71210fbef775"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RandomForest -> VAL  RMSE: 19.075, MAE: 8.519\n",
            "RandomForest -> TEST RMSE: 21.334, MAE: 8.777\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2.2. XGBoost**"
      ],
      "metadata": {
        "id": "UVAOBrzCapXJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "xgb = XGBRegressor(\n",
        "    n_estimators=50,\n",
        "    max_depth=3,\n",
        "    learning_rate=0.3,\n",
        "    subsample=0.5,\n",
        "    colsample_bytree=0.5,\n",
        "    reg_lambda=5,\n",
        "    random_state=42,\n",
        "    n_jobs=-1,\n",
        ")\n",
        "evaluate_baseline_full(\"XGBoost\", xgb,\n",
        "                       X_train, y_train,\n",
        "                       X_val, y_val,\n",
        "                       X_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4M1vf9gdao95",
        "outputId": "62e2d46d-f070-4174-9c46-c038f6adaf18"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XGBoost -> VAL  RMSE: 18.061, MAE: 5.860\n",
            "XGBoost -> TEST RMSE: 20.404, MAE: 6.061\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3. MLP Deep Learning Baseline Model**"
      ],
      "metadata": {
        "id": "CN2g072UekBs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3.1. Import libraries and download data**"
      ],
      "metadata": {
        "id": "3SaLz1SWe0di"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "DATA_DIR = \"/content/data\"\n",
        "\n",
        "META_PATH = os.path.join(DATA_DIR, \"metadata.json\")\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "lM3piEtVewqj",
        "outputId": "fc36b1d8-17f6-4aca-a5ce-3c978b915cef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cuda'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3.2. Prepare the Airbnb Open Dataset**"
      ],
      "metadata": {
        "id": "7YE7ns-VfuGE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AirbnbDataset(Dataset):\n",
        "    def __init__(self, csv_path: str, meta_path: str):\n",
        "        self.df = pd.read_csv(csv_path)\n",
        "\n",
        "        with open(meta_path, \"r\") as f:\n",
        "            meta = json.load(f)\n",
        "\n",
        "        self.target_col = meta[\"target_col\"]\n",
        "        self.numeric_cols = meta[\"numeric_cols\"]\n",
        "        self.categorical_cols = meta[\"categorical_cols\"]\n",
        "\n",
        "        self.y = self.df[self.target_col].values.astype(\"float32\")\n",
        "\n",
        "        if self.numeric_cols:\n",
        "            self.x_num = self.df[self.numeric_cols].values.astype(\"float32\")\n",
        "        else:\n",
        "            self.x_num = None\n",
        "\n",
        "        if self.categorical_cols:\n",
        "            self.x_cat = self.df[self.categorical_cols].values.astype(\"int64\")\n",
        "        else:\n",
        "            self.x_cat = None\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        y = torch.tensor(self.y[idx], dtype=torch.float32)\n",
        "\n",
        "        if self.x_num is not None:\n",
        "            x_num = torch.tensor(self.x_num[idx], dtype=torch.float32)\n",
        "        else:\n",
        "            x_num = torch.empty(0, dtype=torch.float32)\n",
        "\n",
        "        if self.x_cat is not None:\n",
        "            x_cat = torch.tensor(self.x_cat[idx], dtype=torch.long)\n",
        "        else:\n",
        "            x_cat = torch.empty(0, dtype=torch.long)\n",
        "\n",
        "        return x_num, x_cat, y"
      ],
      "metadata": {
        "id": "V8GNsBgPf2nU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3.3. MLP Pricing Model**"
      ],
      "metadata": {
        "id": "S2BVRUVaf4rc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MLPPriceModel(nn.Module):\n",
        "    def __init__(self, meta_path: str, embed_dim: int = 8, hidden_dim: int = 256):\n",
        "        super().__init__()\n",
        "\n",
        "        with open(meta_path, \"r\") as f:\n",
        "            meta = json.load(f)\n",
        "\n",
        "        self.numeric_cols = meta[\"numeric_cols\"]\n",
        "        self.categorical_cols = meta[\"categorical_cols\"]\n",
        "        cat_cardinalities = meta[\"cat_cardinalities\"]\n",
        "\n",
        "        self.num_numeric = len(self.numeric_cols)\n",
        "\n",
        "        self.embeds = nn.ModuleList()\n",
        "        for col in self.categorical_cols:\n",
        "            # cat_cardinalities[col] now already includes the +1 for the unknown category\n",
        "            num_categories = cat_cardinalities[col]\n",
        "            self.embeds.append(nn.Embedding(num_categories, embed_dim))\n",
        "\n",
        "        input_dim = self.num_numeric + len(self.categorical_cols) * embed_dim\n",
        "\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim // 2, 1),\n",
        "        )\n",
        "\n",
        "    def forward(self, x_num, x_cat):\n",
        "        embed_list = []\n",
        "        for i, emb in enumerate(self.embeds):\n",
        "            embed_list.append(emb(x_cat[:, i]))  # [B, embed_dim]\n",
        "\n",
        "        if embed_list:\n",
        "            x_embed = torch.cat(embed_list, dim=1)\n",
        "            x = torch.cat([x_num, x_embed], dim=1)\n",
        "        else:\n",
        "            x = x_num\n",
        "\n",
        "        out = self.mlp(x).squeeze(1)  # [B]\n",
        "        return out"
      ],
      "metadata": {
        "id": "axfVX2TFgINJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3.4. Train and Test function**"
      ],
      "metadata": {
        "id": "PEWDr8p3gNlK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "import numpy as np\n",
        "\n",
        "def train_one_epoch(model, loader, criterion, optimizer, device):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    n = 0\n",
        "\n",
        "    for x_num, x_cat, y in loader:\n",
        "        x_num = x_num.to(device)\n",
        "        x_cat = x_cat.to(device)\n",
        "        y = y.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        preds = model(x_num, x_cat)\n",
        "        loss = criterion(preds, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        bs = y.size(0)\n",
        "        total_loss += loss.item() * bs\n",
        "        n += bs\n",
        "\n",
        "    return total_loss / n\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate(model, loader, criterion, device):\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    n = 0\n",
        "\n",
        "    for x_num, x_cat, y in loader:\n",
        "        x_num = x_num.to(device)\n",
        "        x_cat = x_cat.to(device)\n",
        "        y = y.to(device)\n",
        "\n",
        "        preds = model(x_num, x_cat)\n",
        "        loss = criterion(preds, y)\n",
        "\n",
        "        bs = y.size(0)\n",
        "        total_loss += loss.item() * bs\n",
        "        n += bs\n",
        "\n",
        "    return total_loss / n\n",
        "\n",
        "@torch.no_grad()\n",
        "def eval_with_metrics(model, loader, device):\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_targets = []\n",
        "\n",
        "    for x_num, x_cat, y in loader:\n",
        "        x_num = x_num.to(device)\n",
        "        x_cat = x_cat.to(device)\n",
        "        y = y.to(device)\n",
        "\n",
        "        preds = model(x_num, x_cat)\n",
        "\n",
        "        all_preds.append(preds.detach().cpu().numpy())\n",
        "        all_targets.append(y.detach().cpu().numpy())\n",
        "\n",
        "    all_preds = np.concatenate(all_preds, axis=0)\n",
        "    all_targets = np.concatenate(all_targets, axis=0)\n",
        "\n",
        "    mse = mean_squared_error(all_targets, all_preds)\n",
        "    rmse = np.sqrt(mse)\n",
        "    mae = mean_absolute_error(all_targets, all_preds)\n",
        "\n",
        "    return rmse, mae"
      ],
      "metadata": {
        "id": "HPit1wK-hnm_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3.5. Load Data and Train Baseline MLP**"
      ],
      "metadata": {
        "id": "LNl7a94Qhsft"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 256\n",
        "epochs = 20\n",
        "lr = 1e-3\n",
        "embed_dim = 8\n",
        "hidden_dim = 256\n",
        "\n",
        "train_csv = os.path.join(DATA_DIR, \"train_processed.csv\")\n",
        "val_csv   = os.path.join(DATA_DIR, \"val_processed.csv\")\n",
        "\n",
        "train_ds = AirbnbDataset(train_csv, META_PATH)\n",
        "val_ds   = AirbnbDataset(val_csv, META_PATH)\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
        "val_loader   = DataLoader(val_ds, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "model = MLPPriceModel(META_PATH, embed_dim=embed_dim, hidden_dim=hidden_dim).to(device)\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "best_val = float(\"inf\")\n",
        "save_dir = \"/content/experiments/outputs\"\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "save_path = os.path.join(save_dir, \"best_mlp.pt\")\n",
        "\n",
        "for epoch in range(1, epochs + 1):\n",
        "    train_loss = train_one_epoch(model, train_loader, criterion, optimizer, device)\n",
        "    val_loss = evaluate(model, val_loader, criterion, device)\n",
        "\n",
        "    print(f\"Epoch {epoch:02d} | train_loss={train_loss:.4f} | val_loss={val_loss:.4f}\")\n",
        "\n",
        "    if val_loss < best_val:\n",
        "        best_val = val_loss\n",
        "        torch.save(model.state_dict(), save_path)\n",
        "        print(f\"  -> New best saved to {save_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3NFR3xSiiQwy",
        "outputId": "02155367-b8d8-4e81-ccb6-a5a4bb2beda5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 01 | train_loss=144077.6325 | val_loss=28877.7676\n",
            "  -> New best saved to /content/experiments/outputs/best_mlp.pt\n",
            "Epoch 02 | train_loss=4869.8252 | val_loss=396.2497\n",
            "  -> New best saved to /content/experiments/outputs/best_mlp.pt\n",
            "Epoch 03 | train_loss=608.8866 | val_loss=330.2882\n",
            "  -> New best saved to /content/experiments/outputs/best_mlp.pt\n",
            "Epoch 04 | train_loss=554.5795 | val_loss=308.4163\n",
            "  -> New best saved to /content/experiments/outputs/best_mlp.pt\n",
            "Epoch 05 | train_loss=523.1757 | val_loss=301.2410\n",
            "  -> New best saved to /content/experiments/outputs/best_mlp.pt\n",
            "Epoch 06 | train_loss=513.9164 | val_loss=307.5439\n",
            "Epoch 07 | train_loss=511.2929 | val_loss=309.4115\n",
            "Epoch 08 | train_loss=504.6627 | val_loss=309.1200\n",
            "Epoch 09 | train_loss=509.9840 | val_loss=301.4132\n",
            "Epoch 10 | train_loss=507.1584 | val_loss=294.0808\n",
            "  -> New best saved to /content/experiments/outputs/best_mlp.pt\n",
            "Epoch 11 | train_loss=506.1570 | val_loss=301.3789\n",
            "Epoch 12 | train_loss=500.8074 | val_loss=296.6047\n",
            "Epoch 13 | train_loss=502.6885 | val_loss=294.4634\n",
            "Epoch 14 | train_loss=502.5831 | val_loss=304.7296\n",
            "Epoch 15 | train_loss=506.4926 | val_loss=303.4732\n",
            "Epoch 16 | train_loss=504.6922 | val_loss=313.2097\n",
            "Epoch 17 | train_loss=499.9213 | val_loss=297.0281\n",
            "Epoch 18 | train_loss=499.6014 | val_loss=308.7612\n",
            "Epoch 19 | train_loss=499.0116 | val_loss=294.9748\n",
            "Epoch 20 | train_loss=502.0128 | val_loss=312.9331\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3.6. Evaluate Baseline MLP**"
      ],
      "metadata": {
        "id": "cspKSpXRl2KP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_csv = os.path.join(DATA_DIR, \"test_processed.csv\")\n",
        "test_ds = AirbnbDataset(test_csv, META_PATH)\n",
        "test_loader = DataLoader(test_ds, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "best_model = MLPPriceModel(META_PATH, embed_dim=embed_dim, hidden_dim=hidden_dim).to(device)\n",
        "best_model.load_state_dict(torch.load(save_path, map_location=device))\n",
        "\n",
        "val_rmse, val_mae = eval_with_metrics(best_model, val_loader, device)\n",
        "print(f\"Best MLP on VAL -> RMSE: {val_rmse:.3f}, MAE: {val_mae:.3f}\")\n",
        "\n",
        "test_rmse, test_mae = eval_with_metrics(best_model, test_loader, device)\n",
        "print(f\"Best MLP on TEST -> RMSE: {test_rmse:.3f}, MAE: {test_mae:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pYIZyNcMl9uk",
        "outputId": "343f6023-6809-4bcc-b88c-c626ee2cd516"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best MLP on VAL -> RMSE: 17.149, MAE: 3.530\n",
            "Best MLP on TEST -> RMSE: 20.007, MAE: 3.665\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **4. log_price**"
      ],
      "metadata": {
        "id": "CaS9C3_rAsnp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "def preprocess_airbnb(raw_path=RAW_PATH, out_dir=OUT_DIR):\n",
        "    df = pd.read_csv(raw_path)\n",
        "\n",
        "    # Standardize column names: trim spaces, convert to lowercase, replace spaces with underscores\n",
        "    df.columns = [c.strip().lower().replace(\" \", \"_\") for c in df.columns]\n",
        "\n",
        "    # Keep only the columns we need\n",
        "    cols_keep = [\n",
        "        \"price\",\n",
        "        \"service_fee\",\n",
        "        \"room_type\",\n",
        "        \"neighbourhood_group\",\n",
        "        \"neighbourhood\",\n",
        "        \"minimum_nights\",\n",
        "        \"number_of_reviews\",\n",
        "        \"reviews_per_month\",\n",
        "        \"calculated_host_listings_count\",\n",
        "        \"availability_365\",\n",
        "        \"review_rate_number\",\n",
        "        \"lat\",\n",
        "        \"long\",\n",
        "        \"cancellation_policy\",\n",
        "        \"host_identity_verified\",\n",
        "        \"instant_bookable\",\n",
        "        \"construction_year\",\n",
        "        \"country\",\n",
        "        \"country_code\",\n",
        "    ]\n",
        "    cols_keep = [c for c in cols_keep if c in df.columns]\n",
        "    df = df[cols_keep].copy()\n",
        "\n",
        "    target_col = \"price\"\n",
        "\n",
        "    # Clean 'price' and 'service_fee' columns\n",
        "    for col in [target_col, 'service_fee']:\n",
        "        if col in df.columns:\n",
        "            df[col] = df[col].astype(str).str.replace('$', '', regex=False).str.replace(',', '', regex=False)\n",
        "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "\n",
        "    # Basic cleaning: drop rows where price is missing or non-positive\n",
        "    df = df.dropna(subset=[target_col])\n",
        "    df = df[df[target_col] > 0]\n",
        "    df[\"log_price\"] = np.log(df[\"price\"])\n",
        "    target_col = \"log_price\"\n",
        "\n",
        "    numeric_cols = [\n",
        "        \"service_fee\",\n",
        "        \"minimum_nights\",\n",
        "        \"number_of_reviews\",\n",
        "        \"reviews_per_month\",\n",
        "        \"calculated_host_listings_count\",\n",
        "        \"availability_365\",\n",
        "        \"review_rate_number\",\n",
        "        \"lat\",\n",
        "        \"long\",\n",
        "        \"construction_year\",\n",
        "    ]\n",
        "    numeric_cols = [c for c in numeric_cols if c in df.columns]\n",
        "\n",
        "    # Fill NaN values in numeric columns (Median imputation)\n",
        "    for col in numeric_cols:\n",
        "        if df[col].isnull().any():\n",
        "            median_val = df[col].median()\n",
        "            df[col] = df[col].fillna(median_val)\n",
        "            # If median is also NaN (empty column), fill with 0\n",
        "            df[col] = df[col].fillna(0)\n",
        "\n",
        "    categorical_cols = [\n",
        "        \"room_type\",\n",
        "        \"neighbourhood_group\",\n",
        "        \"neighbourhood\",\n",
        "        \"cancellation_policy\",\n",
        "        \"host_identity_verified\",\n",
        "        \"instant_bookable\",\n",
        "        \"country\",\n",
        "        \"country_code\",\n",
        "    ]\n",
        "    categorical_cols = [c for c in categorical_cols if c in df.columns]\n",
        "\n",
        "    # Fill NaN values in categorical columns before encoding\n",
        "    for col in categorical_cols:\n",
        "        if col in df.columns and df[col].isnull().any():\n",
        "            df[col] = df[col].fillna('__MISSING__').astype(str)\n",
        "\n",
        "    # Train/validation/test split\n",
        "    train_df, temp_df = train_test_split(df, test_size=0.2, random_state=42)\n",
        "    val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n",
        "\n",
        "    # Scale numeric features\n",
        "    scaler = StandardScaler()\n",
        "    if numeric_cols:\n",
        "        # Fit on train, transform all\n",
        "        train_df[numeric_cols] = scaler.fit_transform(train_df[numeric_cols])\n",
        "        val_df[numeric_cols] = scaler.transform(val_df[numeric_cols])\n",
        "        test_df[numeric_cols] = scaler.transform(test_df[numeric_cols])\n",
        "\n",
        "    # Ordinal encode categorical features\n",
        "    encoder = OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=-1)\n",
        "\n",
        "    if categorical_cols:\n",
        "        train_cat_encoded = encoder.fit_transform(train_df[categorical_cols])\n",
        "        val_cat_encoded = encoder.transform(val_df[categorical_cols])\n",
        "        test_cat_encoded = encoder.transform(test_df[categorical_cols])\n",
        "\n",
        "        # Map -1 (unknown) to 0, and shift all other categories by 1\n",
        "        train_df[categorical_cols] = np.where(train_cat_encoded == -1, 0, train_cat_encoded + 1).astype(int)\n",
        "        val_df[categorical_cols] = np.where(val_cat_encoded == -1, 0, val_cat_encoded + 1).astype(int)\n",
        "        test_df[categorical_cols] = np.where(test_cat_encoded == -1, 0, test_cat_encoded + 1).astype(int)\n",
        "\n",
        "    # Save processed datasets\n",
        "    train_path = os.path.join(out_dir, \"train_processed.csv\")\n",
        "    val_path = os.path.join(out_dir, \"val_processed.csv\")\n",
        "    test_path = os.path.join(out_dir, \"test_processed.csv\")\n",
        "\n",
        "    train_df.to_csv(train_path, index=False)\n",
        "    val_df.to_csv(val_path, index=False)\n",
        "    test_df.to_csv(test_path, index=False)\n",
        "\n",
        "    # Save metadata\n",
        "    metadata = {\n",
        "        \"target_col\": target_col,\n",
        "        \"numeric_cols\": numeric_cols,\n",
        "        \"categorical_cols\": categorical_cols,\n",
        "        \"cat_cardinalities\": {},\n",
        "    }\n",
        "\n",
        "    for i, col in enumerate(categorical_cols):\n",
        "        metadata[\"cat_cardinalities\"][col] = len(encoder.categories_[i]) + 1\n",
        "\n",
        "    meta_path = os.path.join(out_dir, \"metadata.json\")\n",
        "    with open(meta_path, \"w\") as f:\n",
        "        json.dump(metadata, f, indent=2)\n",
        "\n",
        "    print(\"Preprocess done.\")\n",
        "    print(f\"Saved: {train_path}\")\n",
        "    print(f\"Saved: {val_path}\")\n",
        "    print(f\"Saved: {test_path}\")\n",
        "    print(f\"Saved: {meta_path}\")"
      ],
      "metadata": {
        "id": "ztihv6XgBA_3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preprocess_airbnb()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sSzfh5bMHaUO",
        "outputId": "0bd4a09c-52dc-4f45-bf49-30fe7ddea125"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-869123102.py:5: DtypeWarning: Columns (25) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df = pd.read_csv(raw_path)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preprocess done.\n",
            "Saved: /content/data/train_processed.csv\n",
            "Saved: /content/data/val_processed.csv\n",
            "Saved: /content/data/test_processed.csv\n",
            "Saved: /content/data/metadata.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **4.1. Import libraries and download data**"
      ],
      "metadata": {
        "id": "TsOZPC7oDvn3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# Directory where preprocessing stored all output files\n",
        "DATA_DIR = \"/content/data\"   # Must match OUT_DIR from preprocessing notebook\n",
        "\n",
        "META_PATH = os.path.join(DATA_DIR, \"metadata.json\")\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "PIPkIrhDD1cA",
        "outputId": "6e0897d9-b42f-4b84-d286-952deafec5ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cpu'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **4.2. Prepare the Airbnb Open Dataset**"
      ],
      "metadata": {
        "id": "Yjy-HqXHD5ao"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AirbnbDataset(Dataset):\n",
        "    def __init__(self, csv_path: str, meta_path: str):\n",
        "        self.df = pd.read_csv(csv_path)\n",
        "\n",
        "        with open(meta_path, \"r\") as f:\n",
        "            meta = json.load(f)\n",
        "\n",
        "        self.target_col = meta[\"target_col\"]\n",
        "        self.numeric_cols = meta[\"numeric_cols\"]\n",
        "        self.categorical_cols = meta[\"categorical_cols\"]\n",
        "\n",
        "        self.y = self.df[self.target_col].values.astype(\"float32\")\n",
        "\n",
        "        if self.numeric_cols:\n",
        "            self.x_num = self.df[self.numeric_cols].values.astype(\"float32\")\n",
        "        else:\n",
        "            self.x_num = None\n",
        "\n",
        "        if self.categorical_cols:\n",
        "            self.x_cat = self.df[self.categorical_cols].values.astype(\"int64\")\n",
        "        else:\n",
        "            self.x_cat = None\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        y = torch.tensor(self.y[idx], dtype=torch.float32)\n",
        "\n",
        "        if self.x_num is not None:\n",
        "            x_num = torch.tensor(self.x_num[idx], dtype=torch.float32)\n",
        "        else:\n",
        "            x_num = torch.empty(0, dtype=torch.float32)\n",
        "\n",
        "        if self.x_cat is not None:\n",
        "            x_cat = torch.tensor(self.x_cat[idx], dtype=torch.long)\n",
        "        else:\n",
        "            x_cat = torch.empty(0, dtype=torch.long)\n",
        "\n",
        "        return x_num, x_cat, y"
      ],
      "metadata": {
        "id": "RIZvRl5-D4kM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **4.3. MLP Pricing Model**"
      ],
      "metadata": {
        "id": "jLRL6H9sEIfQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MLPPriceModel(nn.Module):\n",
        "    def __init__(self, meta_path: str, embed_dim: int = 8, hidden_dim: int = 256):\n",
        "        super().__init__()\n",
        "\n",
        "        with open(meta_path, \"r\") as f:\n",
        "            meta = json.load(f)\n",
        "\n",
        "        self.numeric_cols = meta[\"numeric_cols\"]\n",
        "        self.categorical_cols = meta[\"categorical_cols\"]\n",
        "        cat_cardinalities = meta[\"cat_cardinalities\"]\n",
        "\n",
        "        self.num_numeric = len(self.numeric_cols)\n",
        "\n",
        "        self.embeds = nn.ModuleList()\n",
        "        for col in self.categorical_cols:\n",
        "            # cat_cardinalities[col] now already includes the +1 for the unknown category\n",
        "            num_categories = cat_cardinalities[col]\n",
        "            self.embeds.append(nn.Embedding(num_categories, embed_dim))\n",
        "\n",
        "        input_dim = self.num_numeric + len(self.categorical_cols) * embed_dim\n",
        "\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim // 2, 1),\n",
        "        )\n",
        "\n",
        "    def forward(self, x_num, x_cat):\n",
        "        embed_list = []\n",
        "        for i, emb in enumerate(self.embeds):\n",
        "            embed_list.append(emb(x_cat[:, i]))  # [B, embed_dim]\n",
        "\n",
        "        if embed_list:\n",
        "            x_embed = torch.cat(embed_list, dim=1)\n",
        "            x = torch.cat([x_num, x_embed], dim=1)\n",
        "        else:\n",
        "            x = x_num\n",
        "\n",
        "        out = self.mlp(x).squeeze(1)  # [B]\n",
        "        return out"
      ],
      "metadata": {
        "id": "v3iR3A4fEawc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **4.4. Train and Test function**"
      ],
      "metadata": {
        "id": "PKYEneW-Enqw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "import numpy as np\n",
        "\n",
        "def train_one_epoch(model, loader, criterion, optimizer, device):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    n = 0\n",
        "\n",
        "    for x_num, x_cat, y in loader:\n",
        "        x_num = x_num.to(device)\n",
        "        x_cat = x_cat.to(device)\n",
        "        y = y.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        preds = model(x_num, x_cat)\n",
        "        loss = criterion(preds, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        bs = y.size(0)\n",
        "        total_loss += loss.item() * bs\n",
        "        n += bs\n",
        "\n",
        "    return total_loss / n\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate(model, loader, criterion, device):\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    n = 0\n",
        "\n",
        "    for x_num, x_cat, y in loader:\n",
        "        x_num = x_num.to(device)\n",
        "        x_cat = x_cat.to(device)\n",
        "        y = y.to(device)\n",
        "\n",
        "        preds = model(x_num, x_cat)\n",
        "        loss = criterion(preds, y)\n",
        "\n",
        "        bs = y.size(0)\n",
        "        total_loss += loss.item() * bs\n",
        "        n += bs\n",
        "\n",
        "    return total_loss / n\n",
        "\n",
        "@torch.no_grad()\n",
        "def eval_with_metrics(model, loader, device):\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_targets = []\n",
        "\n",
        "    for x_num, x_cat, y in loader:\n",
        "        x_num = x_num.to(device)\n",
        "        x_cat = x_cat.to(device)\n",
        "        y = y.to(device)\n",
        "\n",
        "        preds = model(x_num, x_cat)\n",
        "\n",
        "        all_preds.append(preds.detach().cpu().numpy())\n",
        "        all_targets.append(y.detach().cpu().numpy())\n",
        "\n",
        "    all_preds = np.concatenate(all_preds, axis=0)\n",
        "    all_targets = np.concatenate(all_targets, axis=0)\n",
        "\n",
        "    all_preds = np.exp(all_preds)\n",
        "    all_targets = np.exp(all_targets)\n",
        "\n",
        "    mse = mean_squared_error(all_targets, all_preds)\n",
        "    rmse = np.sqrt(mse)\n",
        "    mae = mean_absolute_error(all_targets, all_preds)\n",
        "\n",
        "    return rmse, mae"
      ],
      "metadata": {
        "id": "CgrAmrFuE2LK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **4.5. Load Data and Train Baseline MLP**"
      ],
      "metadata": {
        "id": "6gJmV-t3Ef1-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 256\n",
        "epochs = 20\n",
        "lr = 1e-3\n",
        "embed_dim = 8\n",
        "hidden_dim = 256\n",
        "\n",
        "train_csv = os.path.join(DATA_DIR, \"train_processed.csv\")\n",
        "val_csv   = os.path.join(DATA_DIR, \"val_processed.csv\")\n",
        "\n",
        "train_ds = AirbnbDataset(train_csv, META_PATH)\n",
        "val_ds   = AirbnbDataset(val_csv, META_PATH)\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
        "val_loader   = DataLoader(val_ds, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "model = MLPPriceModel(META_PATH, embed_dim=embed_dim, hidden_dim=hidden_dim).to(device)\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "best_val = float(\"inf\")\n",
        "save_dir = \"/content/experiments/outputs\"\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "save_path = os.path.join(save_dir, \"best_mlp.pt\")\n",
        "\n",
        "for epoch in range(1, epochs + 1):\n",
        "    train_loss = train_one_epoch(model, train_loader, criterion, optimizer, device)\n",
        "    val_loss = evaluate(model, val_loader, criterion, device)\n",
        "\n",
        "    print(f\"Epoch {epoch:02d} | train_loss={train_loss:.4f} | val_loss={val_loss:.4f}\")\n",
        "\n",
        "    if val_loss < best_val:\n",
        "        best_val = val_loss\n",
        "        torch.save(model.state_dict(), save_path)\n",
        "        print(f\"  -> New best saved to {save_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HSlgdtqEEyTh",
        "outputId": "8280853b-57b1-46e3-9b83-7b8cd07148a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 01 | train_loss=1.1727 | val_loss=0.0811\n",
            "  -> New best saved to /content/experiments/outputs/best_mlp.pt\n",
            "Epoch 02 | train_loss=0.0911 | val_loss=0.0201\n",
            "  -> New best saved to /content/experiments/outputs/best_mlp.pt\n",
            "Epoch 03 | train_loss=0.0513 | val_loss=0.0132\n",
            "  -> New best saved to /content/experiments/outputs/best_mlp.pt\n",
            "Epoch 04 | train_loss=0.0409 | val_loss=0.0059\n",
            "  -> New best saved to /content/experiments/outputs/best_mlp.pt\n",
            "Epoch 05 | train_loss=0.0367 | val_loss=0.0079\n",
            "Epoch 06 | train_loss=0.0326 | val_loss=0.0051\n",
            "  -> New best saved to /content/experiments/outputs/best_mlp.pt\n",
            "Epoch 07 | train_loss=0.0306 | val_loss=0.0074\n",
            "Epoch 08 | train_loss=0.0287 | val_loss=0.0057\n",
            "Epoch 09 | train_loss=0.0266 | val_loss=0.0038\n",
            "  -> New best saved to /content/experiments/outputs/best_mlp.pt\n",
            "Epoch 10 | train_loss=0.0256 | val_loss=0.0078\n",
            "Epoch 11 | train_loss=0.0247 | val_loss=0.0047\n",
            "Epoch 12 | train_loss=0.0247 | val_loss=0.0034\n",
            "  -> New best saved to /content/experiments/outputs/best_mlp.pt\n",
            "Epoch 13 | train_loss=0.0220 | val_loss=0.0034\n",
            "  -> New best saved to /content/experiments/outputs/best_mlp.pt\n",
            "Epoch 14 | train_loss=0.0216 | val_loss=0.0160\n",
            "Epoch 15 | train_loss=0.0197 | val_loss=0.0051\n",
            "Epoch 16 | train_loss=0.0175 | val_loss=0.0642\n",
            "Epoch 17 | train_loss=0.0151 | val_loss=0.0675\n",
            "Epoch 18 | train_loss=0.0111 | val_loss=0.0841\n",
            "Epoch 19 | train_loss=0.0090 | val_loss=0.0878\n",
            "Epoch 20 | train_loss=0.0081 | val_loss=0.1160\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **4.6. Evaluate Baseline MLP**"
      ],
      "metadata": {
        "id": "8056QW1YE-j3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_csv = os.path.join(DATA_DIR, \"test_processed.csv\")\n",
        "test_ds = AirbnbDataset(test_csv, META_PATH)\n",
        "test_loader = DataLoader(test_ds, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "best_model = MLPPriceModel(META_PATH, embed_dim=embed_dim, hidden_dim=hidden_dim).to(device)\n",
        "best_model.load_state_dict(torch.load(save_path, map_location=device))\n",
        "\n",
        "val_rmse, val_mae = eval_with_metrics(best_model, val_loader, device)\n",
        "print(f\"Best MLP on VAL -> RMSE: {val_rmse:.3f}, MAE: {val_mae:.3f}\")\n",
        "\n",
        "test_rmse, test_mae = eval_with_metrics(best_model, test_loader, device)\n",
        "print(f\"Best MLP on TEST -> RMSE: {test_rmse:.3f}, MAE: {test_mae:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b0Lt8Re5FB0f",
        "outputId": "984cdc4a-7a48-47d9-86d2-69ead344fe11"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best MLP on VAL -> RMSE: 32.173, MAE: 20.038\n",
            "Best MLP on TEST -> RMSE: 33.283, MAE: 20.094\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **5. Large MLP vs Small MLP**"
      ],
      "metadata": {
        "id": "Y4Y5glx_V1jk"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dec27f78"
      },
      "source": [
        "class MLPPriceModel(nn.Module):\n",
        "    def __init__(self, meta_path: str, embed_dim: int = 8, hidden_dim: int = 256, dropout: float = 0.1):\n",
        "        super().__init__()\n",
        "\n",
        "        with open(meta_path, \"r\") as f:\n",
        "            meta = json.load(f)\n",
        "\n",
        "        self.numeric_cols = meta[\"numeric_cols\"]\n",
        "        self.categorical_cols = meta[\"categorical_cols\"]\n",
        "        cat_cardinalities = meta[\"cat_cardinalities\"]\n",
        "\n",
        "        self.num_numeric = len(self.numeric_cols)\n",
        "\n",
        "        self.embeds = nn.ModuleList()\n",
        "        for col in self.categorical_cols:\n",
        "            # cat_cardinalities[col] now already includes the +1 for the unknown category\n",
        "            num_categories = cat_cardinalities[col]\n",
        "            self.embeds.append(nn.Embedding(num_categories, embed_dim))\n",
        "\n",
        "        input_dim = self.num_numeric + len(self.categorical_cols) * embed_dim\n",
        "\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim // 2, 1),\n",
        "        )\n",
        "\n",
        "    def forward(self, x_num, x_cat):\n",
        "        embed_list = []\n",
        "        for i, emb in enumerate(self.embeds):\n",
        "            embed_list.append(emb(x_cat[:, i]))  # [B, embed_dim]\n",
        "\n",
        "        if embed_list:\n",
        "            x_embed = torch.cat(embed_list, dim=1)\n",
        "            x = torch.cat([x_num, x_embed], dim=1)\n",
        "        else:\n",
        "            x = x_num\n",
        "\n",
        "        out = self.mlp(x).squeeze(1)  # [B]\n",
        "        return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4032cb03"
      },
      "source": [
        "## **5.1. Train Small MLP**\n",
        "\n",
        "### Subtask:\n",
        "Train the 'Small MLP' configuration (embed_dim=4, hidden_dim=128, dropout=0.05) and evaluate its performance.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "525e2599"
      },
      "source": [
        "**Reasoning**:\n",
        "Train the Small MLP model with the specified hyperparameters, save the best model, and evaluate it on validation and test sets.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "98154882",
        "outputId": "7ae1a689-2045-4910-ce7f-a475ee40b881"
      },
      "source": [
        "# Hyperparameters for Small MLP\n",
        "embed_dim_small = 4\n",
        "hidden_dim_small = 128\n",
        "dropout_small = 0.05\n",
        "epochs = 20\n",
        "lr = 1e-3\n",
        "batch_size = 256\n",
        "\n",
        "# Re-initialize Loaders (to ensure consistency)\n",
        "train_ds = AirbnbDataset(os.path.join(DATA_DIR, \"train_processed.csv\"), META_PATH)\n",
        "val_ds   = AirbnbDataset(os.path.join(DATA_DIR, \"val_processed.csv\"), META_PATH)\n",
        "test_ds  = AirbnbDataset(os.path.join(DATA_DIR, \"test_processed.csv\"), META_PATH)\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
        "val_loader   = DataLoader(val_ds, batch_size=batch_size, shuffle=False)\n",
        "test_loader  = DataLoader(test_ds, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Initialize Model\n",
        "model_small = MLPPriceModel(\n",
        "    meta_path=META_PATH,\n",
        "    embed_dim=embed_dim_small,\n",
        "    hidden_dim=hidden_dim_small,\n",
        "    dropout=dropout_small\n",
        ").to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(model_small.parameters(), lr=lr)\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "# Training Loop\n",
        "save_path_small = os.path.join(save_dir, \"small_mlp.pt\")\n",
        "best_val_small = float(\"inf\")\n",
        "\n",
        "print(\"Training Small MLP...\")\n",
        "for epoch in range(1, epochs + 1):\n",
        "    train_loss = train_one_epoch(model_small, train_loader, criterion, optimizer, device)\n",
        "    val_loss = evaluate(model_small, val_loader, criterion, device)\n",
        "\n",
        "    print(f\"Epoch {epoch:02d} | train_loss={train_loss:.4f} | val_loss={val_loss:.4f}\")\n",
        "\n",
        "    if val_loss < best_val_small:\n",
        "        best_val_small = val_loss\n",
        "        torch.save(model_small.state_dict(), save_path_small)\n",
        "        print(f\"  -> New best saved to {save_path_small}\")\n",
        "\n",
        "print(\"\\nEvaluating Best Small MLP...\")\n",
        "model_small.load_state_dict(torch.load(save_path_small, map_location=device))\n",
        "\n",
        "val_rmse, val_mae = eval_with_metrics(model_small, val_loader, device)\n",
        "print(f\"Small MLP on VAL  -> RMSE: {val_rmse:.3f}, MAE: {val_mae:.3f}\")\n",
        "\n",
        "test_rmse, test_mae = eval_with_metrics(model_small, test_loader, device)\n",
        "print(f\"Small MLP on TEST -> RMSE: {test_rmse:.3f}, MAE: {test_mae:.3f}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Small MLP...\n",
            "Epoch 01 | train_loss=195501.0435 | val_loss=47075.7379\n",
            "  -> New best saved to /content/experiments/outputs/small_mlp.pt\n",
            "Epoch 02 | train_loss=15763.8485 | val_loss=808.2934\n",
            "  -> New best saved to /content/experiments/outputs/small_mlp.pt\n",
            "Epoch 03 | train_loss=724.1164 | val_loss=383.7133\n",
            "  -> New best saved to /content/experiments/outputs/small_mlp.pt\n",
            "Epoch 04 | train_loss=574.6233 | val_loss=337.0288\n",
            "  -> New best saved to /content/experiments/outputs/small_mlp.pt\n",
            "Epoch 05 | train_loss=529.4603 | val_loss=312.9179\n",
            "  -> New best saved to /content/experiments/outputs/small_mlp.pt\n",
            "Epoch 06 | train_loss=510.0393 | val_loss=301.3764\n",
            "  -> New best saved to /content/experiments/outputs/small_mlp.pt\n",
            "Epoch 07 | train_loss=496.8661 | val_loss=297.0272\n",
            "  -> New best saved to /content/experiments/outputs/small_mlp.pt\n",
            "Epoch 08 | train_loss=486.7819 | val_loss=292.6822\n",
            "  -> New best saved to /content/experiments/outputs/small_mlp.pt\n",
            "Epoch 09 | train_loss=484.2217 | val_loss=290.3350\n",
            "  -> New best saved to /content/experiments/outputs/small_mlp.pt\n",
            "Epoch 10 | train_loss=479.2161 | val_loss=288.9813\n",
            "  -> New best saved to /content/experiments/outputs/small_mlp.pt\n",
            "Epoch 11 | train_loss=481.0155 | val_loss=288.9005\n",
            "  -> New best saved to /content/experiments/outputs/small_mlp.pt\n",
            "Epoch 12 | train_loss=478.9374 | val_loss=287.9952\n",
            "  -> New best saved to /content/experiments/outputs/small_mlp.pt\n",
            "Epoch 13 | train_loss=476.4395 | val_loss=289.3790\n",
            "Epoch 14 | train_loss=475.7108 | val_loss=296.1654\n",
            "Epoch 15 | train_loss=477.0174 | val_loss=289.3010\n",
            "Epoch 16 | train_loss=476.7978 | val_loss=288.2952\n",
            "Epoch 17 | train_loss=472.4686 | val_loss=291.6867\n",
            "Epoch 18 | train_loss=475.1323 | val_loss=295.7778\n",
            "Epoch 19 | train_loss=469.9833 | val_loss=292.8547\n",
            "Epoch 20 | train_loss=479.7259 | val_loss=305.2731\n",
            "\n",
            "Evaluating Best Small MLP...\n",
            "Small MLP on VAL  -> RMSE: 16.970, MAE: 2.545\n",
            "Small MLP on TEST -> RMSE: 19.811, MAE: 2.728\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3c153c0c"
      },
      "source": [
        "## **5.2. Train Large MLP**\n",
        "\n",
        "### Subtask:\n",
        "Train the 'Large MLP' configuration (embed_dim=16, hidden_dim=512, dropout=0.2) and evaluate its performance.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ec873173"
      },
      "source": [
        "**Reasoning**:\n",
        "Train the Large MLP model with the specified hyperparameters, save the best model, and evaluate it on validation and test sets.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "34d6be7a",
        "outputId": "6ccf7956-13ae-4b28-a4ab-ab628ed8098a"
      },
      "source": [
        "# Hyperparameters for Large MLP\n",
        "embed_dim_large = 16\n",
        "hidden_dim_large = 512\n",
        "dropout_large = 0.2\n",
        "epochs = 20\n",
        "lr = 1e-3\n",
        "\n",
        "# Initialize Model\n",
        "model_large = MLPPriceModel(\n",
        "    meta_path=META_PATH,\n",
        "    embed_dim=embed_dim_large,\n",
        "    hidden_dim=hidden_dim_large,\n",
        "    dropout=dropout_large\n",
        ").to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(model_large.parameters(), lr=lr)\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "# Training Loop\n",
        "save_path_large = os.path.join(save_dir, \"large_mlp.pt\")\n",
        "best_val_large = float(\"inf\")\n",
        "\n",
        "print(\"Training Large MLP...\")\n",
        "for epoch in range(1, epochs + 1):\n",
        "    train_loss = train_one_epoch(model_large, train_loader, criterion, optimizer, device)\n",
        "    val_loss = evaluate(model_large, val_loader, criterion, device)\n",
        "\n",
        "    print(f\"Epoch {epoch:02d} | train_loss={train_loss:.4f} | val_loss={val_loss:.4f}\")\n",
        "\n",
        "    if val_loss < best_val_large:\n",
        "        best_val_large = val_loss\n",
        "        torch.save(model_large.state_dict(), save_path_large)\n",
        "        print(f\"  -> New best saved to {save_path_large}\")\n",
        "\n",
        "print(\"\\nEvaluating Best Large MLP...\")\n",
        "model_large.load_state_dict(torch.load(save_path_large, map_location=device))\n",
        "\n",
        "val_rmse, val_mae = eval_with_metrics(model_large, val_loader, device)\n",
        "print(f\"Large MLP on VAL  -> RMSE: {val_rmse:.3f}, MAE: {val_mae:.3f}\")\n",
        "\n",
        "test_rmse, test_mae = eval_with_metrics(model_large, test_loader, device)\n",
        "print(f\"Large MLP on TEST -> RMSE: {test_rmse:.3f}, MAE: {test_mae:.3f}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Large MLP...\n",
            "Epoch 01 | train_loss=100110.5215 | val_loss=1359.9446\n",
            "  -> New best saved to /content/experiments/outputs/large_mlp.pt\n",
            "Epoch 02 | train_loss=805.2411 | val_loss=359.7810\n",
            "  -> New best saved to /content/experiments/outputs/large_mlp.pt\n",
            "Epoch 03 | train_loss=668.2381 | val_loss=384.9090\n",
            "Epoch 04 | train_loss=639.8300 | val_loss=317.8292\n",
            "  -> New best saved to /content/experiments/outputs/large_mlp.pt\n",
            "Epoch 05 | train_loss=622.2850 | val_loss=360.2366\n",
            "Epoch 06 | train_loss=604.9114 | val_loss=312.4567\n",
            "  -> New best saved to /content/experiments/outputs/large_mlp.pt\n",
            "Epoch 07 | train_loss=588.9663 | val_loss=332.2355\n",
            "Epoch 08 | train_loss=584.6105 | val_loss=327.5446\n",
            "Epoch 09 | train_loss=572.4352 | val_loss=334.8095\n",
            "Epoch 10 | train_loss=578.5624 | val_loss=320.2399\n",
            "Epoch 11 | train_loss=579.6523 | val_loss=374.6021\n",
            "Epoch 12 | train_loss=566.7217 | val_loss=355.3954\n",
            "Epoch 13 | train_loss=570.5925 | val_loss=490.4145\n",
            "Epoch 14 | train_loss=560.1618 | val_loss=324.4490\n",
            "Epoch 15 | train_loss=562.9637 | val_loss=303.1468\n",
            "  -> New best saved to /content/experiments/outputs/large_mlp.pt\n",
            "Epoch 16 | train_loss=553.5539 | val_loss=317.1596\n",
            "Epoch 17 | train_loss=554.2041 | val_loss=298.9158\n",
            "  -> New best saved to /content/experiments/outputs/large_mlp.pt\n",
            "Epoch 18 | train_loss=561.9286 | val_loss=306.2979\n",
            "Epoch 19 | train_loss=554.1698 | val_loss=453.6778\n",
            "Epoch 20 | train_loss=554.3608 | val_loss=311.4240\n",
            "\n",
            "Evaluating Best Large MLP...\n",
            "Large MLP on VAL  -> RMSE: 17.289, MAE: 3.925\n",
            "Large MLP on TEST -> RMSE: 20.081, MAE: 4.093\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **6. TF-IDF**"
      ],
      "metadata": {
        "id": "LoFeH4GVY7Sh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "def preprocess_airbnb(raw_path=RAW_PATH, out_dir=OUT_DIR):\n",
        "    df = pd.read_csv(raw_path)\n",
        "\n",
        "    df.columns = [c.strip().lower().replace(\" \", \"_\") for c in df.columns]\n",
        "\n",
        "    cols_keep = [\n",
        "        \"price\",\n",
        "        \"service_fee\",\n",
        "        \"room_type\",\n",
        "        \"neighbourhood_group\",\n",
        "        \"neighbourhood\",\n",
        "        \"minimum_nights\",\n",
        "        \"number_of_reviews\",\n",
        "        \"reviews_per_month\",\n",
        "        \"calculated_host_listings_count\",\n",
        "        \"availability_365\",\n",
        "        \"review_rate_number\",\n",
        "        \"lat\",\n",
        "        \"long\",\n",
        "        \"cancellation_policy\",\n",
        "        \"host_identity_verified\",\n",
        "        \"instant_bookable\",\n",
        "        \"construction_year\",\n",
        "        \"country\",\n",
        "        \"country_code\",\n",
        "        \"name\",\n",
        "        \"description\",\n",
        "    ]\n",
        "    cols_keep = [c for c in cols_keep if c in df.columns]\n",
        "    df = df[cols_keep].copy()\n",
        "\n",
        "    target_col = \"price\"\n",
        "\n",
        "    for col in [target_col, 'service_fee']:\n",
        "        if col in df.columns:\n",
        "            df[col] = df[col].astype(str).str.replace('$', '', regex=False).str.replace(',', '', regex=False)\n",
        "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "\n",
        "    df = df.dropna(subset=[target_col])\n",
        "    df = df[df[target_col] > 0]\n",
        "\n",
        "    numeric_cols = [\n",
        "        \"service_fee\",\n",
        "        \"minimum_nights\",\n",
        "        \"number_of_reviews\",\n",
        "        \"reviews_per_month\",\n",
        "        \"calculated_host_listings_count\",\n",
        "        \"availability_365\",\n",
        "        \"review_rate_number\",\n",
        "        \"lat\",\n",
        "        \"long\",\n",
        "        \"construction_year\",\n",
        "    ]\n",
        "    numeric_cols = [c for c in numeric_cols if c in df.columns]\n",
        "\n",
        "    for col in numeric_cols:\n",
        "        if df[col].isnull().any():\n",
        "            median_val = df[col].median()\n",
        "            df[col] = df[col].fillna(median_val)\n",
        "        df[col] = df[col].fillna(0)\n",
        "\n",
        "    categorical_cols = [\n",
        "        \"room_type\",\n",
        "        \"neighbourhood_group\",\n",
        "        \"neighbourhood\",\n",
        "        \"cancellation_policy\",\n",
        "        \"host_identity_verified\",\n",
        "        \"instant_bookable\",\n",
        "        \"country\",\n",
        "        \"country_code\",\n",
        "    ]\n",
        "    categorical_cols = [c for c in categorical_cols if c in df.columns]\n",
        "\n",
        "    for col in categorical_cols:\n",
        "        if df[col].isnull().any():\n",
        "            df[col] = df[col].fillna(\"__MISSING__\").astype(str)\n",
        "\n",
        "    text_cols = []\n",
        "    if \"description\" in df.columns:\n",
        "        text_cols.append(\"description\")\n",
        "    if \"name\" in df.columns:\n",
        "        text_cols.append(\"name\")\n",
        "    if \"neighbourhood\" in df.columns:\n",
        "        text_cols.append(\"neighbourhood\")\n",
        "    if \"neighbourhood_group\" in df.columns:\n",
        "        text_cols.append(\"neighbourhood_group\")\n",
        "\n",
        "    if len(text_cols) == 0:\n",
        "        text_series = pd.Series([\"\"] * len(df))\n",
        "    else:\n",
        "        text_series = df[text_cols].fillna(\"\").astype(str).agg(\" \".join, axis=1)\n",
        "\n",
        "    tfidf = TfidfVectorizer(max_features=5000)\n",
        "    tfidf_matrix = tfidf.fit_transform(text_series)\n",
        "\n",
        "    pca = PCA(n_components=16, random_state=42)\n",
        "    text_embeddings = pca.fit_transform(tfidf_matrix.toarray())\n",
        "\n",
        "    text_embed_path = os.path.join(out_dir, \"text_embeddings.npy\")\n",
        "    np.save(text_embed_path, text_embeddings)\n",
        "\n",
        "    # ----------------------------\n",
        "\n",
        "    train_df, temp_df, train_text, temp_text = train_test_split(\n",
        "        df, text_embeddings, test_size=0.2, random_state=42\n",
        "    )\n",
        "    val_df, test_df, val_text, test_text = train_test_split(\n",
        "        temp_df, temp_text, test_size=0.5, random_state=42\n",
        "    )\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    if numeric_cols:\n",
        "        train_df[numeric_cols] = scaler.fit_transform(train_df[numeric_cols])\n",
        "        val_df[numeric_cols] = scaler.transform(val_df[numeric_cols])\n",
        "        test_df[numeric_cols] = scaler.transform(test_df[numeric_cols])\n",
        "\n",
        "    encoder = OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=-1)\n",
        "\n",
        "    if categorical_cols:\n",
        "        train_cat_encoded = encoder.fit_transform(train_df[categorical_cols])\n",
        "        val_cat_encoded = encoder.transform(val_df[categorical_cols])\n",
        "        test_cat_encoded = encoder.transform(test_df[categorical_cols])\n",
        "\n",
        "        train_df[categorical_cols] = np.where(train_cat_encoded == -1, 0, train_cat_encoded + 1).astype(int)\n",
        "        val_df[categorical_cols] = np.where(val_cat_encoded == -1, 0, val_cat_encoded + 1).astype(int)\n",
        "        test_df[categorical_cols] = np.where(test_cat_encoded == -1, 0, test_cat_encoded + 1).astype(int)\n",
        "\n",
        "    train_df.to_csv(os.path.join(out_dir, \"train_processed.csv\"), index=False)\n",
        "    val_df.to_csv(os.path.join(out_dir, \"val_processed.csv\"), index=False)\n",
        "    test_df.to_csv(os.path.join(out_dir, \"test_processed.csv\"), index=False)\n",
        "\n",
        "    metadata = {\n",
        "        \"target_col\": target_col,\n",
        "        \"numeric_cols\": numeric_cols,\n",
        "        \"categorical_cols\": categorical_cols,\n",
        "        \"text_dim\": 16,\n",
        "        \"cat_cardinalities\": {},\n",
        "    }\n",
        "\n",
        "    for i, col in enumerate(categorical_cols):\n",
        "        metadata[\"cat_cardinalities\"][col] = len(encoder.categories_[i]) + 1\n",
        "\n",
        "    with open(os.path.join(out_dir, \"metadata.json\"), \"w\") as f:\n",
        "        json.dump(metadata, f, indent=2)\n",
        "\n",
        "    print(\"Preprocess done.\")\n",
        "    print(\"Saved:\", out_dir)\n"
      ],
      "metadata": {
        "id": "QVPDO3tqY6yQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preprocess_airbnb()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-2REPKjrZP_T",
        "outputId": "048eee96-b5d4-4d8e-ae3f-240848bceb0b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1688658603.py:7: DtypeWarning: Columns (25) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df = pd.read_csv(raw_path)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preprocess done.\n",
            "Saved: /content/data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 256\n",
        "epochs = 20\n",
        "lr = 1e-3\n",
        "embed_dim = 8\n",
        "hidden_dim = 256\n",
        "\n",
        "train_csv = os.path.join(DATA_DIR, \"train_processed.csv\")\n",
        "val_csv   = os.path.join(DATA_DIR, \"val_processed.csv\")\n",
        "\n",
        "train_ds = AirbnbDataset(train_csv, META_PATH)\n",
        "val_ds   = AirbnbDataset(val_csv, META_PATH)\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
        "val_loader   = DataLoader(val_ds, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "model = MLPPriceModel(META_PATH, embed_dim=embed_dim, hidden_dim=hidden_dim).to(device)\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "best_val = float(\"inf\")\n",
        "save_dir = \"/content/experiments/outputs\"\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "save_path = os.path.join(save_dir, \"best_mlp.pt\")\n",
        "\n",
        "for epoch in range(1, epochs + 1):\n",
        "    train_loss = train_one_epoch(model, train_loader, criterion, optimizer, device)\n",
        "    val_loss = evaluate(model, val_loader, criterion, device)\n",
        "\n",
        "    print(f\"Epoch {epoch:02d} | train_loss={train_loss:.4f} | val_loss={val_loss:.4f}\")\n",
        "\n",
        "    if val_loss < best_val:\n",
        "        best_val = val_loss\n",
        "        torch.save(model.state_dict(), save_path)\n",
        "        print(f\"  -> New best saved to {save_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O_jMzgyyZN9W",
        "outputId": "724e99ca-ad06-4bef-ec23-473d30592902"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 01 | train_loss=131489.5060 | val_loss=22394.7534\n",
            "  -> New best saved to /content/experiments/outputs/best_mlp.pt\n",
            "Epoch 02 | train_loss=3286.8720 | val_loss=412.0780\n",
            "  -> New best saved to /content/experiments/outputs/best_mlp.pt\n",
            "Epoch 03 | train_loss=622.4379 | val_loss=322.4845\n",
            "  -> New best saved to /content/experiments/outputs/best_mlp.pt\n",
            "Epoch 04 | train_loss=553.3055 | val_loss=312.8251\n",
            "  -> New best saved to /content/experiments/outputs/best_mlp.pt\n",
            "Epoch 05 | train_loss=535.1397 | val_loss=298.0162\n",
            "  -> New best saved to /content/experiments/outputs/best_mlp.pt\n",
            "Epoch 06 | train_loss=530.9735 | val_loss=313.5283\n",
            "Epoch 07 | train_loss=511.4739 | val_loss=319.2828\n",
            "Epoch 08 | train_loss=517.2073 | val_loss=299.2469\n",
            "Epoch 09 | train_loss=511.8141 | val_loss=304.5762\n",
            "Epoch 10 | train_loss=515.3220 | val_loss=313.5430\n",
            "Epoch 11 | train_loss=513.1058 | val_loss=314.2175\n",
            "Epoch 12 | train_loss=514.6342 | val_loss=307.1279\n",
            "Epoch 13 | train_loss=503.3811 | val_loss=297.7880\n",
            "  -> New best saved to /content/experiments/outputs/best_mlp.pt\n",
            "Epoch 14 | train_loss=511.1570 | val_loss=292.3526\n",
            "  -> New best saved to /content/experiments/outputs/best_mlp.pt\n",
            "Epoch 15 | train_loss=508.3278 | val_loss=299.9113\n",
            "Epoch 16 | train_loss=508.5597 | val_loss=303.7885\n",
            "Epoch 17 | train_loss=503.5918 | val_loss=289.7311\n",
            "  -> New best saved to /content/experiments/outputs/best_mlp.pt\n",
            "Epoch 18 | train_loss=504.0511 | val_loss=303.1826\n",
            "Epoch 19 | train_loss=506.7427 | val_loss=304.7587\n",
            "Epoch 20 | train_loss=503.6541 | val_loss=346.8847\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_csv = os.path.join(DATA_DIR, \"test_processed.csv\")\n",
        "test_ds = AirbnbDataset(test_csv, META_PATH)\n",
        "test_loader = DataLoader(test_ds, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "best_model = MLPPriceModel(META_PATH, embed_dim=embed_dim, hidden_dim=hidden_dim).to(device)\n",
        "best_model.load_state_dict(torch.load(save_path, map_location=device))\n",
        "\n",
        "val_rmse, val_mae = eval_with_metrics(best_model, val_loader, device)\n",
        "print(f\"Best MLP on VAL -> RMSE: {val_rmse:.3f}, MAE: {val_mae:.3f}\")\n",
        "\n",
        "test_rmse, test_mae = eval_with_metrics(best_model, test_loader, device)\n",
        "print(f\"Best MLP on TEST -> RMSE: {test_rmse:.3f}, MAE: {test_mae:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xpv7wDYkZPDJ",
        "outputId": "e2523dd2-8427-4d7d-a492-2d9d94f79c6e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best MLP on VAL -> RMSE: 17.021, MAE: 2.992\n",
            "Best MLP on TEST -> RMSE: 19.855, MAE: 3.217\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ece58e7"
      },
      "source": [
        "# **7. Deeper MLP with BatchNorm & Dropout**\n",
        "\n",
        "### Subtask:\n",
        "Define the DeepMLPPriceModel class, train it on the prepared data, and evaluate performance.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5e6e9e72"
      },
      "source": [
        "Define a new `DeepMLPPriceModel` class featuring a deeper architecture (3-4 layers), Batch Normalization layers, and increased Dropout (0.2-0.3). Train this model on the processed dataset located in `\"/content/data\"` and evaluate its RMSE and MAE on validation and test sets to verify architectural improvements."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bfacf540"
      },
      "source": [
        "**Reasoning**:\n",
        "Define the DeepMLPPriceModel class with the specified architecture (BatchNorm, Dropout) and train it using the Airbnb dataset. Then evaluate it on validation and test sets without applying exponential transformation to the predictions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d2f85ec3",
        "outputId": "6ef763bf-ff8d-44e2-dd66-43c311b85249"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import pandas as pd\n",
        "import json\n",
        "import os\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "\n",
        "# Define Model Class\n",
        "class DeepMLPPriceModel(nn.Module):\n",
        "    def __init__(self, meta_path, embed_dim=8, hidden_dim=256, dropout=0.3):\n",
        "        super().__init__()\n",
        "        with open(meta_path, \"r\") as f:\n",
        "            meta = json.load(f)\n",
        "\n",
        "        self.numeric_cols = meta[\"numeric_cols\"]\n",
        "        self.categorical_cols = meta[\"categorical_cols\"]\n",
        "        cat_cardinalities = meta[\"cat_cardinalities\"]\n",
        "\n",
        "        self.embeds = nn.ModuleList()\n",
        "        for col in self.categorical_cols:\n",
        "            self.embeds.append(nn.Embedding(cat_cardinalities[col], embed_dim))\n",
        "\n",
        "        input_dim = len(self.numeric_cols) + len(self.categorical_cols) * embed_dim\n",
        "\n",
        "        # Hidden Block 1\n",
        "        self.block1 = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden_dim),\n",
        "            nn.BatchNorm1d(hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "        # Hidden Block 2\n",
        "        self.block2 = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
        "            nn.BatchNorm1d(hidden_dim // 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "        # Hidden Block 3\n",
        "        self.block3 = nn.Sequential(\n",
        "            nn.Linear(hidden_dim // 2, hidden_dim // 4),\n",
        "            nn.BatchNorm1d(hidden_dim // 4),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "        # Output Layer\n",
        "        self.output = nn.Linear(hidden_dim // 4, 1)\n",
        "\n",
        "    def forward(self, x_num, x_cat):\n",
        "        embed_list = []\n",
        "        for i, emb in enumerate(self.embeds):\n",
        "            embed_list.append(emb(x_cat[:, i]))\n",
        "\n",
        "        if embed_list:\n",
        "            x_cat_embed = torch.cat(embed_list, dim=1)\n",
        "            x = torch.cat([x_num, x_cat_embed], dim=1)\n",
        "        else:\n",
        "            x = x_num\n",
        "\n",
        "        x = self.block1(x)\n",
        "        x = self.block2(x)\n",
        "        x = self.block3(x)\n",
        "        out = self.output(x).squeeze(1)\n",
        "        return out\n",
        "\n",
        "# Setup Data and Components\n",
        "DATA_DIR = \"/content/data\"\n",
        "META_PATH = os.path.join(DATA_DIR, \"metadata.json\")\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "batch_size = 256\n",
        "lr = 1e-3\n",
        "epochs = 20\n",
        "\n",
        "train_ds = AirbnbDataset(os.path.join(DATA_DIR, \"train_processed.csv\"), META_PATH)\n",
        "val_ds = AirbnbDataset(os.path.join(DATA_DIR, \"val_processed.csv\"), META_PATH)\n",
        "test_ds = AirbnbDataset(os.path.join(DATA_DIR, \"test_processed.csv\"), META_PATH)\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False)\n",
        "test_loader = DataLoader(test_ds, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "model = DeepMLPPriceModel(META_PATH).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "# Training Loop\n",
        "save_dir = \"/content/experiments/outputs\"\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "best_path = os.path.join(save_dir, \"best_deep_mlp.pt\")\n",
        "best_val_loss = float(\"inf\")\n",
        "\n",
        "print(\"Training Deep MLP with BatchNorm & Dropout...\")\n",
        "\n",
        "for epoch in range(1, epochs + 1):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    n = 0\n",
        "    for x_num, x_cat, y in train_loader:\n",
        "        x_num, x_cat, y = x_num.to(device), x_cat.to(device), y.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        preds = model(x_num, x_cat)\n",
        "        loss = criterion(preds, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item() * y.size(0)\n",
        "        n += y.size(0)\n",
        "    train_loss = total_loss / n\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    val_loss_accum = 0\n",
        "    n_val = 0\n",
        "    with torch.no_grad():\n",
        "        for x_num, x_cat, y in val_loader:\n",
        "            x_num, x_cat, y = x_num.to(device), x_cat.to(device), y.to(device)\n",
        "            preds = model(x_num, x_cat)\n",
        "            loss = criterion(preds, y)\n",
        "            val_loss_accum += loss.item() * y.size(0)\n",
        "            n_val += y.size(0)\n",
        "    val_loss = val_loss_accum / n_val\n",
        "\n",
        "    print(f\"Epoch {epoch:02d} | train_loss={train_loss:.4f} | val_loss={val_loss:.4f}\")\n",
        "\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        torch.save(model.state_dict(), best_path)\n",
        "        print(f\"  -> New best saved to {best_path}\")\n",
        "\n",
        "# Evaluation\n",
        "print(\"\\nEvaluating Best Deep MLP...\")\n",
        "model.load_state_dict(torch.load(best_path))\n",
        "model.eval()\n",
        "\n",
        "def get_metrics(loader):\n",
        "    all_preds = []\n",
        "    all_targets = []\n",
        "    with torch.no_grad():\n",
        "        for x_num, x_cat, y in loader:\n",
        "            x_num, x_cat, y = x_num.to(device), x_cat.to(device), y.to(device)\n",
        "            preds = model(x_num, x_cat)\n",
        "            all_preds.append(preds.cpu().numpy())\n",
        "            all_targets.append(y.cpu().numpy())\n",
        "\n",
        "    all_preds = np.concatenate(all_preds)\n",
        "    all_targets = np.concatenate(all_targets)\n",
        "\n",
        "    mse = mean_squared_error(all_targets, all_preds)\n",
        "    rmse = np.sqrt(mse)\n",
        "    mae = mean_absolute_error(all_targets, all_preds)\n",
        "    return rmse, mae\n",
        "\n",
        "val_rmse, val_mae = get_metrics(val_loader)\n",
        "print(f\"Deep MLP on VAL  -> RMSE: {val_rmse:.3f}, MAE: {val_mae:.3f}\")\n",
        "\n",
        "test_rmse, test_mae = get_metrics(test_loader)\n",
        "print(f\"Deep MLP on TEST -> RMSE: {test_rmse:.3f}, MAE: {test_mae:.3f}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Deep MLP with BatchNorm & Dropout...\n",
            "Epoch 01 | train_loss=484291.9940 | val_loss=457367.2498\n",
            "  -> New best saved to /content/experiments/outputs/best_deep_mlp.pt\n",
            "Epoch 02 | train_loss=417280.8405 | val_loss=369281.1189\n",
            "  -> New best saved to /content/experiments/outputs/best_deep_mlp.pt\n",
            "Epoch 03 | train_loss=316427.9462 | val_loss=266591.9071\n",
            "  -> New best saved to /content/experiments/outputs/best_deep_mlp.pt\n",
            "Epoch 04 | train_loss=209917.2974 | val_loss=154866.3736\n",
            "  -> New best saved to /content/experiments/outputs/best_deep_mlp.pt\n",
            "Epoch 05 | train_loss=121481.2638 | val_loss=83054.7934\n",
            "  -> New best saved to /content/experiments/outputs/best_deep_mlp.pt\n",
            "Epoch 06 | train_loss=62014.4670 | val_loss=35674.8625\n",
            "  -> New best saved to /content/experiments/outputs/best_deep_mlp.pt\n",
            "Epoch 07 | train_loss=29551.1230 | val_loss=15454.3059\n",
            "  -> New best saved to /content/experiments/outputs/best_deep_mlp.pt\n",
            "Epoch 08 | train_loss=15374.6996 | val_loss=8748.3724\n",
            "  -> New best saved to /content/experiments/outputs/best_deep_mlp.pt\n",
            "Epoch 09 | train_loss=9951.4880 | val_loss=2967.5901\n",
            "  -> New best saved to /content/experiments/outputs/best_deep_mlp.pt\n",
            "Epoch 10 | train_loss=8178.0877 | val_loss=2089.9706\n",
            "  -> New best saved to /content/experiments/outputs/best_deep_mlp.pt\n",
            "Epoch 11 | train_loss=6615.2393 | val_loss=807.1372\n",
            "  -> New best saved to /content/experiments/outputs/best_deep_mlp.pt\n",
            "Epoch 12 | train_loss=5951.0009 | val_loss=813.4870\n",
            "Epoch 13 | train_loss=5728.3512 | val_loss=887.9647\n",
            "Epoch 14 | train_loss=5583.5126 | val_loss=384.7410\n",
            "  -> New best saved to /content/experiments/outputs/best_deep_mlp.pt\n",
            "Epoch 15 | train_loss=5584.1059 | val_loss=524.8068\n",
            "Epoch 16 | train_loss=5410.0869 | val_loss=643.8243\n",
            "Epoch 17 | train_loss=5399.4597 | val_loss=461.2932\n",
            "Epoch 18 | train_loss=5395.8224 | val_loss=594.6461\n",
            "Epoch 19 | train_loss=5360.0281 | val_loss=389.2352\n",
            "Epoch 20 | train_loss=5293.7519 | val_loss=610.5936\n",
            "\n",
            "Evaluating Best Deep MLP...\n",
            "Deep MLP on VAL  -> RMSE: 19.615, MAE: 7.967\n",
            "Deep MLP on TEST -> RMSE: 22.311, MAE: 8.166\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "be7578d8"
      },
      "source": [
        "# **8. SmoothL1 Loss Experiment**\n",
        "\n",
        "### Subtask:\n",
        "Retrain the baseline MLP architecture using SmoothL1Loss to improve robustness to outliers.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0d14d06b"
      },
      "source": [
        "**Reasoning**:\n",
        "I will perform the SmoothL1 Loss experiment by defining the baseline MLP model, initializing it with the SmoothL1Loss function, training it for 20 epochs, and evaluating its performance using RMSE and MAE metrics on the validation and test sets.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4eae26e6",
        "outputId": "cd8f1b4f-5f75-4ccc-d232-f526db6cb16c"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import json\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "\n",
        "DATA_DIR = \"/content/data\"\n",
        "META_PATH = os.path.join(DATA_DIR, \"metadata.json\")\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "batch_size = 256\n",
        "lr = 1e-3\n",
        "epochs = 20\n",
        "\n",
        "with open(META_PATH, \"r\") as f:\n",
        "    meta = json.load(f)\n",
        "print(f\"Target column in metadata: {meta['target_col']}\")\n",
        "\n",
        "class AirbnbDataset(Dataset):\n",
        "    def __init__(self, csv_path, meta_path):\n",
        "        self.df = pd.read_csv(csv_path)\n",
        "        with open(meta_path, \"r\") as f:\n",
        "            meta = json.load(f)\n",
        "        self.target_col = meta[\"target_col\"]\n",
        "        self.numeric_cols = meta[\"numeric_cols\"]\n",
        "        self.categorical_cols = meta[\"categorical_cols\"]\n",
        "\n",
        "        self.y = self.df[self.target_col].values.astype(\"float32\")\n",
        "        self.x_num = self.df[self.numeric_cols].values.astype(\"float32\") if self.numeric_cols else np.zeros((len(self.df), 0), dtype=\"float32\")\n",
        "        self.x_cat = self.df[self.categorical_cols].values.astype(\"int64\") if self.categorical_cols else np.zeros((len(self.df), 0), dtype=\"int64\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return (\n",
        "            torch.tensor(self.x_num[idx]),\n",
        "            torch.tensor(self.x_cat[idx]),\n",
        "            torch.tensor(self.y[idx])\n",
        "        )\n",
        "\n",
        "# Baseline MLP Model Definition\n",
        "class MLPPriceModel(nn.Module):\n",
        "    def __init__(self, meta_path, embed_dim=8, hidden_dim=256, dropout=0.1):\n",
        "        super().__init__()\n",
        "        with open(meta_path, \"r\") as f:\n",
        "            meta = json.load(f)\n",
        "        self.numeric_cols = meta[\"numeric_cols\"]\n",
        "        self.categorical_cols = meta[\"categorical_cols\"]\n",
        "        cat_cardinalities = meta[\"cat_cardinalities\"]\n",
        "\n",
        "        self.embeds = nn.ModuleList([\n",
        "            nn.Embedding(cat_cardinalities[col], embed_dim)\n",
        "            for col in self.categorical_cols\n",
        "        ])\n",
        "\n",
        "        input_dim = len(self.numeric_cols) + len(self.categorical_cols) * embed_dim\n",
        "\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim // 2, 1),\n",
        "        )\n",
        "\n",
        "    def forward(self, x_num, x_cat):\n",
        "        embs = [emb(x_cat[:, i]) for i, emb in enumerate(self.embeds)]\n",
        "        if embs:\n",
        "            x = torch.cat([x_num] + embs, dim=1)\n",
        "        else:\n",
        "            x = x_num\n",
        "        return self.mlp(x).squeeze(1)\n",
        "\n",
        "# Load Data\n",
        "train_ds = AirbnbDataset(os.path.join(DATA_DIR, \"train_processed.csv\"), META_PATH)\n",
        "val_ds = AirbnbDataset(os.path.join(DATA_DIR, \"val_processed.csv\"), META_PATH)\n",
        "test_ds = AirbnbDataset(os.path.join(DATA_DIR, \"test_processed.csv\"), META_PATH)\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False)\n",
        "test_loader = DataLoader(test_ds, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Initialize Model, Optimizer, and SmoothL1Loss\n",
        "model = MLPPriceModel(META_PATH, embed_dim=8, hidden_dim=256, dropout=0.1).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "criterion = nn.SmoothL1Loss()\n",
        "\n",
        "# Training Loop\n",
        "save_dir = \"/content/experiments/outputs\"\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "save_path = os.path.join(save_dir, \"best_smooth_mlp.pt\")\n",
        "best_val_loss = float(\"inf\")\n",
        "\n",
        "print(\"Training Baseline MLP with SmoothL1Loss...\")\n",
        "\n",
        "for epoch in range(1, epochs + 1):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    n = 0\n",
        "    for x_num, x_cat, y in train_loader:\n",
        "        x_num, x_cat, y = x_num.to(device), x_cat.to(device), y.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        preds = model(x_num, x_cat)\n",
        "        loss = criterion(preds, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item() * y.size(0)\n",
        "        n += y.size(0)\n",
        "\n",
        "    train_loss = total_loss / n\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    total_val_loss = 0\n",
        "    n_val = 0\n",
        "    with torch.no_grad():\n",
        "        for x_num, x_cat, y in val_loader:\n",
        "            x_num, x_cat, y = x_num.to(device), x_cat.to(device), y.to(device)\n",
        "            preds = model(x_num, x_cat)\n",
        "            loss = criterion(preds, y)\n",
        "            total_val_loss += loss.item() * y.size(0)\n",
        "            n_val += y.size(0)\n",
        "\n",
        "    val_loss = total_val_loss / n_val\n",
        "\n",
        "    print(f\"Epoch {epoch:02d} | train_loss={train_loss:.4f} | val_loss={val_loss:.4f}\")\n",
        "\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        torch.save(model.state_dict(), save_path)\n",
        "        print(f\"  -> New best saved to {save_path}\")\n",
        "\n",
        "# Evaluation Function\n",
        "def eval_metrics(model, loader, device):\n",
        "    model.eval()\n",
        "    preds_list, targets_list = [], []\n",
        "    with torch.no_grad():\n",
        "        for x_num, x_cat, y in loader:\n",
        "            x_num, x_cat, y = x_num.to(device), x_cat.to(device), y.to(device)\n",
        "            preds = model(x_num, x_cat)\n",
        "            preds_list.append(preds.cpu().numpy())\n",
        "            targets_list.append(y.cpu().numpy())\n",
        "\n",
        "    preds = np.concatenate(preds_list)\n",
        "    targets = np.concatenate(targets_list)\n",
        "\n",
        "    mse = mean_squared_error(targets, preds)\n",
        "    rmse = np.sqrt(mse)\n",
        "    mae = mean_absolute_error(targets, preds)\n",
        "    return rmse, mae\n",
        "\n",
        "model.load_state_dict(torch.load(save_path, map_location=device))\n",
        "print(\"\\nEvaluating Best SmoothL1 Model...\")\n",
        "\n",
        "val_rmse, val_mae = eval_metrics(model, val_loader, device)\n",
        "print(f\"SmoothL1 MLP on VAL  -> RMSE: {val_rmse:.3f}, MAE: {val_mae:.3f}\")\n",
        "\n",
        "test_rmse, test_mae = eval_metrics(model, test_loader, device)\n",
        "print(f\"SmoothL1 MLP on TEST -> RMSE: {test_rmse:.3f}, MAE: {test_mae:.3f}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Target column in metadata: price\n",
            "Training Baseline MLP with SmoothL1Loss...\n",
            "Epoch 01 | train_loss=276.0808 | val_loss=19.9877\n",
            "  -> New best saved to /content/experiments/outputs/best_smooth_mlp.pt\n",
            "Epoch 02 | train_loss=14.6179 | val_loss=3.5851\n",
            "  -> New best saved to /content/experiments/outputs/best_smooth_mlp.pt\n",
            "Epoch 03 | train_loss=12.2438 | val_loss=3.1626\n",
            "  -> New best saved to /content/experiments/outputs/best_smooth_mlp.pt\n",
            "Epoch 04 | train_loss=11.8735 | val_loss=3.4423\n",
            "Epoch 05 | train_loss=11.6937 | val_loss=3.4223\n",
            "Epoch 06 | train_loss=11.4883 | val_loss=3.5100\n",
            "Epoch 07 | train_loss=11.5093 | val_loss=4.4283\n",
            "Epoch 08 | train_loss=11.3698 | val_loss=2.8783\n",
            "  -> New best saved to /content/experiments/outputs/best_smooth_mlp.pt\n",
            "Epoch 09 | train_loss=11.2770 | val_loss=3.4705\n",
            "Epoch 10 | train_loss=11.2170 | val_loss=3.9305\n",
            "Epoch 11 | train_loss=11.1832 | val_loss=2.6696\n",
            "  -> New best saved to /content/experiments/outputs/best_smooth_mlp.pt\n",
            "Epoch 12 | train_loss=11.1950 | val_loss=3.8837\n",
            "Epoch 13 | train_loss=11.1501 | val_loss=3.1759\n",
            "Epoch 14 | train_loss=11.2222 | val_loss=2.8344\n",
            "Epoch 15 | train_loss=11.2377 | val_loss=3.5027\n",
            "Epoch 16 | train_loss=11.1951 | val_loss=3.2164\n",
            "Epoch 17 | train_loss=11.1173 | val_loss=3.5481\n",
            "Epoch 18 | train_loss=11.0127 | val_loss=4.3699\n",
            "Epoch 19 | train_loss=11.0927 | val_loss=3.6337\n",
            "Epoch 20 | train_loss=11.0787 | val_loss=2.6786\n",
            "\n",
            "Evaluating Best SmoothL1 Model...\n",
            "SmoothL1 MLP on VAL  -> RMSE: 17.046, MAE: 3.126\n",
            "SmoothL1 MLP on TEST -> RMSE: 19.895, MAE: 3.325\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "98255a49"
      },
      "source": [
        "## Final Task\n",
        "\n",
        "### Subtask:\n",
        "Summarize and compare performance metrics of Baseline MLP, Deep MLP, and SmoothL1 MLP.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ae40d92d"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Q&A\n",
        "\n",
        "**How does the performance of the Deep MLP compare to the Baseline MLP trained with SmoothL1 Loss?**\n",
        "\n",
        "The Baseline MLP trained with SmoothL1 Loss significantly outperformed the Deep MLP (trained with standard MSE Loss), particularly in terms of Mean Absolute Error (MAE).\n",
        "*   **Deep MLP (MSE Loss):** Test RMSE: 22.311, Test MAE: 8.166.\n",
        "*   **Baseline MLP (SmoothL1 Loss):** Test RMSE: 19.895, Test MAE: 3.325.\n",
        "\n",
        "While the Deep MLP introduced architectural complexity (3 layers, BatchNorm, higher Dropout), the change in loss function to SmoothL1 provided a much larger gain in accuracy, reducing the MAE by more than half compared to the Deep MLP approach.\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "*   **Deep MLP Architecture Performance:**\n",
        "    *   A deeper model featuring 3 hidden blocks (256 -> 128 -> 64 units), Batch Normalization, and increased Dropout (0.3) was implemented.\n",
        "    *   Training stabilized with a best validation loss achieved at **Epoch 14**.\n",
        "    *   The model achieved a **Validation RMSE of 19.615** and **Test RMSE of 22.311**.\n",
        "    *   The MAE remained relatively high at **8.166** on the test set.\n",
        "\n",
        "*   **SmoothL1 Loss Experiment:**\n",
        "    *   Retraining the baseline MLP architecture using `SmoothL1Loss` (which is less sensitive to outliers than MSE) resulted in rapid convergence, peaking at **Epoch 11**.\n",
        "    *   This configuration achieved the best results among the experiments shown: **Validation RMSE of 17.046** and **Test RMSE of 19.895**.\n",
        "    *   The **Test MAE dropped to 3.325**, indicating much better per-prediction accuracy compared to the Deep MLP's MAE of ~8.17.\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "*   **Loss Function Impact:** The significant drop in MAE (from ~8.17 to ~3.33) when switching to SmoothL1 Loss suggests the dataset contains significant outliers that skew the MSE-based training, regardless of network depth. Robust loss functions are more critical here than architectural depth.\n",
        "*   **Combination Strategy:** The next logical step is to train the **Deep MLP architecture** using **SmoothL1 Loss**. This would test if the architectural improvements (BatchNorm/Depth) can yield even better results when not hindered by the sensitivity of standard MSE loss to outliers.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **9. Combine**"
      ],
      "metadata": {
        "id": "y7L87t0ohNWT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "def preprocess_airbnb(raw_path=RAW_PATH, out_dir=OUT_DIR):\n",
        "    df = pd.read_csv(raw_path)\n",
        "\n",
        "    df.columns = [c.strip().lower().replace(\" \", \"_\") for c in df.columns]\n",
        "\n",
        "    cols_keep = [\n",
        "        \"price\",\n",
        "        \"service_fee\",\n",
        "        \"room_type\",\n",
        "        \"neighbourhood_group\",\n",
        "        \"neighbourhood\",\n",
        "        \"minimum_nights\",\n",
        "        \"number_of_reviews\",\n",
        "        \"reviews_per_month\",\n",
        "        \"calculated_host_listings_count\",\n",
        "        \"availability_365\",\n",
        "        \"review_rate_number\",\n",
        "        \"lat\",\n",
        "        \"long\",\n",
        "        \"cancellation_policy\",\n",
        "        \"host_identity_verified\",\n",
        "        \"instant_bookable\",\n",
        "        \"construction_year\",\n",
        "        \"country\",\n",
        "        \"country_code\",\n",
        "        \"name\",\n",
        "        \"description\",\n",
        "    ]\n",
        "    cols_keep = [c for c in cols_keep if c in df.columns]\n",
        "    df = df[cols_keep].copy()\n",
        "\n",
        "    target_col = \"price\"\n",
        "\n",
        "    for col in [target_col, 'service_fee']:\n",
        "        if col in df.columns:\n",
        "            df[col] = df[col].astype(str).str.replace('$', '', regex=False).str.replace(',', '', regex=False)\n",
        "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "\n",
        "    df = df.dropna(subset=[target_col])\n",
        "    df = df[df[target_col] > 0]\n",
        "\n",
        "    numeric_cols = [\n",
        "        \"service_fee\",\n",
        "        \"minimum_nights\",\n",
        "        \"number_of_reviews\",\n",
        "        \"reviews_per_month\",\n",
        "        \"calculated_host_listings_count\",\n",
        "        \"availability_365\",\n",
        "        \"review_rate_number\",\n",
        "        \"lat\",\n",
        "        \"long\",\n",
        "        \"construction_year\",\n",
        "    ]\n",
        "    numeric_cols = [c for c in numeric_cols if c in df.columns]\n",
        "\n",
        "    for col in numeric_cols:\n",
        "        if df[col].isnull().any():\n",
        "            median_val = df[col].median()\n",
        "            df[col] = df[col].fillna(median_val)\n",
        "        df[col] = df[col].fillna(0)\n",
        "\n",
        "    categorical_cols = [\n",
        "        \"room_type\",\n",
        "        \"neighbourhood_group\",\n",
        "        \"neighbourhood\",\n",
        "        \"cancellation_policy\",\n",
        "        \"host_identity_verified\",\n",
        "        \"instant_bookable\",\n",
        "        \"country\",\n",
        "        \"country_code\",\n",
        "    ]\n",
        "    categorical_cols = [c for c in categorical_cols if c in df.columns]\n",
        "\n",
        "    for col in categorical_cols:\n",
        "        if df[col].isnull().any():\n",
        "            df[col] = df[col].fillna(\"__MISSING__\").astype(str)\n",
        "\n",
        "    # ----------------------------\n",
        "    # TEXT FEATURE PIPELINE (safe)\n",
        "    # ----------------------------\n",
        "    text_cols = []\n",
        "    if \"description\" in df.columns:\n",
        "        text_cols.append(\"description\")\n",
        "    if \"name\" in df.columns:\n",
        "        text_cols.append(\"name\")\n",
        "    if \"neighbourhood\" in df.columns:\n",
        "        text_cols.append(\"neighbourhood\")\n",
        "    if \"neighbourhood_group\" in df.columns:\n",
        "        text_cols.append(\"neighbourhood_group\")\n",
        "\n",
        "    if len(text_cols) == 0:\n",
        "        text_series = pd.Series([\"\"] * len(df))\n",
        "    else:\n",
        "        text_series = df[text_cols].fillna(\"\").astype(str).agg(\" \".join, axis=1)\n",
        "\n",
        "    tfidf = TfidfVectorizer(max_features=5000)\n",
        "    tfidf_matrix = tfidf.fit_transform(text_series)\n",
        "\n",
        "    pca = PCA(n_components=16, random_state=42)\n",
        "    text_embeddings = pca.fit_transform(tfidf_matrix.toarray())\n",
        "\n",
        "    text_embed_path = os.path.join(out_dir, \"text_embeddings.npy\")\n",
        "    np.save(text_embed_path, text_embeddings)\n",
        "\n",
        "\n",
        "    train_df, temp_df, train_text, temp_text = train_test_split(\n",
        "        df, text_embeddings, test_size=0.2, random_state=42\n",
        "    )\n",
        "    val_df, test_df, val_text, test_text = train_test_split(\n",
        "        temp_df, temp_text, test_size=0.5, random_state=42\n",
        "    )\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    if numeric_cols:\n",
        "        train_df[numeric_cols] = scaler.fit_transform(train_df[numeric_cols])\n",
        "        val_df[numeric_cols] = scaler.transform(val_df[numeric_cols])\n",
        "        test_df[numeric_cols] = scaler.transform(test_df[numeric_cols])\n",
        "\n",
        "    encoder = OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=-1)\n",
        "\n",
        "    if categorical_cols:\n",
        "        train_cat_encoded = encoder.fit_transform(train_df[categorical_cols])\n",
        "        val_cat_encoded = encoder.transform(val_df[categorical_cols])\n",
        "        test_cat_encoded = encoder.transform(test_df[categorical_cols])\n",
        "\n",
        "        train_df[categorical_cols] = np.where(train_cat_encoded == -1, 0, train_cat_encoded + 1).astype(int)\n",
        "        val_df[categorical_cols] = np.where(val_cat_encoded == -1, 0, val_cat_encoded + 1).astype(int)\n",
        "        test_df[categorical_cols] = np.where(test_cat_encoded == -1, 0, test_cat_encoded + 1).astype(int)\n",
        "\n",
        "    train_df.to_csv(os.path.join(out_dir, \"train_processed.csv\"), index=False)\n",
        "    val_df.to_csv(os.path.join(out_dir, \"val_processed.csv\"), index=False)\n",
        "    test_df.to_csv(os.path.join(out_dir, \"test_processed.csv\"), index=False)\n",
        "\n",
        "    metadata = {\n",
        "        \"target_col\": target_col,\n",
        "        \"numeric_cols\": numeric_cols,\n",
        "        \"categorical_cols\": categorical_cols,\n",
        "        \"text_dim\": 16,\n",
        "        \"cat_cardinalities\": {},\n",
        "    }\n",
        "\n",
        "    for i, col in enumerate(categorical_cols):\n",
        "        metadata[\"cat_cardinalities\"][col] = len(encoder.categories_[i]) + 1\n",
        "\n",
        "    with open(os.path.join(out_dir, \"metadata.json\"), \"w\") as f:\n",
        "        json.dump(metadata, f, indent=2)\n",
        "\n",
        "    print(\"Preprocess done.\")\n",
        "    print(\"Saved:\", out_dir)\n",
        "\n",
        "preprocess_airbnb()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dobrbmk_hSkf",
        "outputId": "a7bdf8f2-65e1-47a3-f2fc-b22d9b165f77"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2930866217.py:7: DtypeWarning: Columns (25) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df = pd.read_csv(raw_path)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preprocess done.\n",
            "Saved: /content/data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ClmB3-khZ43"
      },
      "source": [
        "class MLPPriceModel(nn.Module):\n",
        "    def __init__(self, meta_path: str, embed_dim: int = 8, hidden_dim: int = 256, dropout: float = 0.1):\n",
        "        super().__init__()\n",
        "\n",
        "        with open(meta_path, \"r\") as f:\n",
        "            meta = json.load(f)\n",
        "\n",
        "        self.numeric_cols = meta[\"numeric_cols\"]\n",
        "        self.categorical_cols = meta[\"categorical_cols\"]\n",
        "        cat_cardinalities = meta[\"cat_cardinalities\"]\n",
        "\n",
        "        self.num_numeric = len(self.numeric_cols)\n",
        "\n",
        "        self.embeds = nn.ModuleList()\n",
        "        for col in self.categorical_cols:\n",
        "            # cat_cardinalities[col] now already includes the +1 for the unknown category\n",
        "            num_categories = cat_cardinalities[col]\n",
        "            self.embeds.append(nn.Embedding(num_categories, embed_dim))\n",
        "\n",
        "        input_dim = self.num_numeric + len(self.categorical_cols) * embed_dim\n",
        "\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim // 2, 1),\n",
        "        )\n",
        "\n",
        "    def forward(self, x_num, x_cat):\n",
        "        embed_list = []\n",
        "        for i, emb in enumerate(self.embeds):\n",
        "            embed_list.append(emb(x_cat[:, i]))  # [B, embed_dim]\n",
        "\n",
        "        if embed_list:\n",
        "            x_embed = torch.cat(embed_list, dim=1)\n",
        "            x = torch.cat([x_num, x_embed], dim=1)\n",
        "        else:\n",
        "            x = x_num\n",
        "\n",
        "        out = self.mlp(x).squeeze(1)  # [B]\n",
        "        return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ziw7yVR-hZ43"
      },
      "source": [
        "## **9.1. Train Small MLP**\n",
        "\n",
        "### Subtask:\n",
        "Train the 'Small MLP' configuration (embed_dim=4, hidden_dim=128, dropout=0.05) and evaluate its performance.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MuaMpf_DhZ43"
      },
      "source": [
        "**Reasoning**:\n",
        "Train the Small MLP model with the specified hyperparameters, save the best model, and evaluate it on validation and test sets.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c2f559f3-2abb-4dd0-d510-69c47fc5ad8b",
        "id": "Jw1DvOGPhZ43"
      },
      "source": [
        "# Hyperparameters for Small MLP\n",
        "embed_dim_small = 4\n",
        "hidden_dim_small = 128\n",
        "dropout_small = 0.05\n",
        "epochs = 100\n",
        "lr = 1e-4\n",
        "batch_size = 32\n",
        "\n",
        "# Re-initialize Loaders (to ensure consistency)\n",
        "train_ds = AirbnbDataset(os.path.join(DATA_DIR, \"train_processed.csv\"), META_PATH)\n",
        "val_ds   = AirbnbDataset(os.path.join(DATA_DIR, \"val_processed.csv\"), META_PATH)\n",
        "test_ds  = AirbnbDataset(os.path.join(DATA_DIR, \"test_processed.csv\"), META_PATH)\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
        "val_loader   = DataLoader(val_ds, batch_size=batch_size, shuffle=False)\n",
        "test_loader  = DataLoader(test_ds, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Initialize Model\n",
        "model_small = MLPPriceModel(\n",
        "    meta_path=META_PATH,\n",
        "    embed_dim=embed_dim_small,\n",
        "    hidden_dim=hidden_dim_small,\n",
        "    dropout=dropout_small\n",
        ").to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(model_small.parameters(), lr=lr)\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "# Training Loop\n",
        "save_path_small = os.path.join(save_dir, \"small_mlp.pt\")\n",
        "best_val_small = float(\"inf\")\n",
        "\n",
        "print(\"Training Small MLP...\")\n",
        "for epoch in range(1, epochs + 1):\n",
        "    train_loss = train_one_epoch(model_small, train_loader, criterion, optimizer, device)\n",
        "    val_loss = evaluate(model_small, val_loader, criterion, device)\n",
        "\n",
        "    print(f\"Epoch {epoch:02d} | train_loss={train_loss:.4f} | val_loss={val_loss:.4f}\")\n",
        "\n",
        "    if val_loss < best_val_small:\n",
        "        best_val_small = val_loss\n",
        "        torch.save(model_small.state_dict(), save_path_small)\n",
        "        print(f\"  -> New best saved to {save_path_small}\")\n",
        "\n",
        "print(\"\\nEvaluating Best Small MLP...\")\n",
        "model_small.load_state_dict(torch.load(save_path_small, map_location=device))\n",
        "\n",
        "val_rmse, val_mae = eval_with_metrics(model_small, val_loader, device)\n",
        "print(f\"Small MLP on VAL  -> RMSE: {val_rmse:.3f}, MAE: {val_mae:.3f}\")\n",
        "\n",
        "test_rmse, test_mae = eval_with_metrics(model_small, test_loader, device)\n",
        "print(f\"Small MLP on TEST -> RMSE: {test_rmse:.3f}, MAE: {test_mae:.3f}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Small MLP...\n",
            "Epoch 01 | train_loss=183948.9628 | val_loss=66130.8807\n",
            "  -> New best saved to /content/experiments/outputs/small_mlp.pt\n",
            "Epoch 02 | train_loss=33656.3332 | val_loss=4235.9072\n",
            "  -> New best saved to /content/experiments/outputs/small_mlp.pt\n",
            "Epoch 03 | train_loss=1372.4933 | val_loss=528.4409\n",
            "  -> New best saved to /content/experiments/outputs/small_mlp.pt\n",
            "Epoch 04 | train_loss=728.8375 | val_loss=390.4685\n",
            "  -> New best saved to /content/experiments/outputs/small_mlp.pt\n",
            "Epoch 05 | train_loss=619.7787 | val_loss=335.8340\n",
            "  -> New best saved to /content/experiments/outputs/small_mlp.pt\n",
            "Epoch 06 | train_loss=564.3281 | val_loss=313.9564\n",
            "  -> New best saved to /content/experiments/outputs/small_mlp.pt\n",
            "Epoch 07 | train_loss=537.8092 | val_loss=303.4005\n",
            "  -> New best saved to /content/experiments/outputs/small_mlp.pt\n",
            "Epoch 08 | train_loss=515.4329 | val_loss=297.2276\n",
            "  -> New best saved to /content/experiments/outputs/small_mlp.pt\n",
            "Epoch 09 | train_loss=507.9888 | val_loss=292.9653\n",
            "  -> New best saved to /content/experiments/outputs/small_mlp.pt\n",
            "Epoch 10 | train_loss=503.9865 | val_loss=292.8190\n",
            "  -> New best saved to /content/experiments/outputs/small_mlp.pt\n",
            "Epoch 11 | train_loss=492.2255 | val_loss=295.5344\n",
            "Epoch 12 | train_loss=487.6320 | val_loss=292.2630\n",
            "  -> New best saved to /content/experiments/outputs/small_mlp.pt\n",
            "Epoch 13 | train_loss=482.5693 | val_loss=288.6747\n",
            "  -> New best saved to /content/experiments/outputs/small_mlp.pt\n",
            "Epoch 14 | train_loss=479.7895 | val_loss=289.3587\n",
            "Epoch 15 | train_loss=478.0601 | val_loss=289.6937\n",
            "Epoch 16 | train_loss=477.4852 | val_loss=288.7546\n",
            "Epoch 17 | train_loss=473.6313 | val_loss=287.7881\n",
            "  -> New best saved to /content/experiments/outputs/small_mlp.pt\n",
            "Epoch 18 | train_loss=476.0508 | val_loss=287.8176\n",
            "Epoch 19 | train_loss=475.2086 | val_loss=290.2504\n",
            "Epoch 20 | train_loss=470.6380 | val_loss=287.2035\n",
            "  -> New best saved to /content/experiments/outputs/small_mlp.pt\n",
            "Epoch 21 | train_loss=471.9028 | val_loss=288.6763\n",
            "Epoch 22 | train_loss=472.6340 | val_loss=289.8702\n",
            "Epoch 23 | train_loss=472.3921 | val_loss=286.6339\n",
            "  -> New best saved to /content/experiments/outputs/small_mlp.pt\n",
            "Epoch 24 | train_loss=466.3564 | val_loss=289.5209\n",
            "Epoch 25 | train_loss=472.4610 | val_loss=288.3865\n",
            "Epoch 26 | train_loss=473.3557 | val_loss=288.1116\n",
            "Epoch 27 | train_loss=464.9517 | val_loss=286.6936\n",
            "Epoch 28 | train_loss=472.2510 | val_loss=284.8723\n",
            "  -> New best saved to /content/experiments/outputs/small_mlp.pt\n",
            "Epoch 29 | train_loss=467.4608 | val_loss=286.0793\n",
            "Epoch 30 | train_loss=466.0786 | val_loss=285.7902\n",
            "Epoch 31 | train_loss=468.2784 | val_loss=287.4660\n",
            "Epoch 32 | train_loss=467.8071 | val_loss=290.5943\n",
            "Epoch 33 | train_loss=470.6429 | val_loss=285.0742\n",
            "Epoch 34 | train_loss=466.9500 | val_loss=287.6762\n",
            "Epoch 35 | train_loss=465.2471 | val_loss=286.2918\n",
            "Epoch 36 | train_loss=463.2038 | val_loss=299.7304\n",
            "Epoch 37 | train_loss=470.4445 | val_loss=286.1114\n",
            "Epoch 38 | train_loss=465.5685 | val_loss=287.7695\n",
            "Epoch 39 | train_loss=466.3733 | val_loss=285.7207\n",
            "Epoch 40 | train_loss=466.4741 | val_loss=287.2878\n",
            "Epoch 41 | train_loss=467.1263 | val_loss=287.3600\n",
            "Epoch 42 | train_loss=466.2553 | val_loss=298.4353\n",
            "Epoch 43 | train_loss=467.0925 | val_loss=286.8836\n",
            "Epoch 44 | train_loss=467.4321 | val_loss=290.7670\n",
            "Epoch 45 | train_loss=465.6905 | val_loss=289.7111\n",
            "Epoch 46 | train_loss=464.6020 | val_loss=286.5535\n",
            "Epoch 47 | train_loss=463.6012 | val_loss=285.5040\n",
            "Epoch 48 | train_loss=462.2171 | val_loss=285.4616\n",
            "Epoch 49 | train_loss=463.1619 | val_loss=286.7345\n",
            "Epoch 50 | train_loss=462.0496 | val_loss=288.9086\n",
            "Epoch 51 | train_loss=465.8770 | val_loss=289.3859\n",
            "Epoch 52 | train_loss=464.2698 | val_loss=286.7222\n",
            "Epoch 53 | train_loss=459.5730 | val_loss=296.3697\n",
            "Epoch 54 | train_loss=461.5469 | val_loss=287.7407\n",
            "Epoch 55 | train_loss=463.3620 | val_loss=286.4985\n",
            "Epoch 56 | train_loss=463.6218 | val_loss=286.4187\n",
            "Epoch 57 | train_loss=463.4623 | val_loss=285.4347\n",
            "Epoch 58 | train_loss=460.1935 | val_loss=287.9209\n",
            "Epoch 59 | train_loss=462.5409 | val_loss=284.7418\n",
            "  -> New best saved to /content/experiments/outputs/small_mlp.pt\n",
            "Epoch 60 | train_loss=463.6451 | val_loss=292.7121\n",
            "Epoch 61 | train_loss=461.4601 | val_loss=289.5480\n",
            "Epoch 62 | train_loss=458.6460 | val_loss=299.8308\n",
            "Epoch 63 | train_loss=462.5209 | val_loss=293.4939\n",
            "Epoch 64 | train_loss=456.7026 | val_loss=288.6652\n",
            "Epoch 65 | train_loss=458.8458 | val_loss=291.6114\n",
            "Epoch 66 | train_loss=462.6670 | val_loss=293.5861\n",
            "Epoch 67 | train_loss=459.4996 | val_loss=287.5118\n",
            "Epoch 68 | train_loss=462.4099 | val_loss=286.7357\n",
            "Epoch 69 | train_loss=455.7234 | val_loss=286.3765\n",
            "Epoch 70 | train_loss=461.6338 | val_loss=294.4392\n",
            "Epoch 71 | train_loss=460.1668 | val_loss=286.2137\n",
            "Epoch 72 | train_loss=459.0699 | val_loss=286.9472\n",
            "Epoch 73 | train_loss=456.6829 | val_loss=285.1030\n",
            "Epoch 74 | train_loss=458.4791 | val_loss=286.9141\n",
            "Epoch 75 | train_loss=461.6726 | val_loss=286.4453\n",
            "Epoch 76 | train_loss=461.7478 | val_loss=285.3361\n",
            "Epoch 77 | train_loss=458.2165 | val_loss=287.0053\n",
            "Epoch 78 | train_loss=457.7779 | val_loss=294.3484\n",
            "Epoch 79 | train_loss=459.5741 | val_loss=287.9194\n",
            "Epoch 80 | train_loss=458.8131 | val_loss=288.6656\n",
            "Epoch 81 | train_loss=459.5110 | val_loss=286.6797\n",
            "Epoch 82 | train_loss=460.8995 | val_loss=291.4021\n",
            "Epoch 83 | train_loss=461.0915 | val_loss=286.4069\n",
            "Epoch 84 | train_loss=458.2750 | val_loss=286.4100\n",
            "Epoch 85 | train_loss=457.3153 | val_loss=284.0612\n",
            "  -> New best saved to /content/experiments/outputs/small_mlp.pt\n",
            "Epoch 86 | train_loss=459.5876 | val_loss=286.4118\n",
            "Epoch 87 | train_loss=463.2992 | val_loss=289.4273\n",
            "Epoch 88 | train_loss=454.1617 | val_loss=296.0923\n",
            "Epoch 89 | train_loss=457.1742 | val_loss=289.0923\n",
            "Epoch 90 | train_loss=457.1612 | val_loss=286.4273\n",
            "Epoch 91 | train_loss=459.9323 | val_loss=285.4816\n",
            "Epoch 92 | train_loss=455.9371 | val_loss=285.8955\n",
            "Epoch 93 | train_loss=456.2904 | val_loss=287.6351\n",
            "Epoch 94 | train_loss=456.7270 | val_loss=288.0372\n",
            "Epoch 95 | train_loss=458.0592 | val_loss=285.4702\n",
            "Epoch 96 | train_loss=459.4025 | val_loss=293.0497\n",
            "Epoch 97 | train_loss=457.1494 | val_loss=288.1356\n",
            "Epoch 98 | train_loss=456.6074 | val_loss=291.8110\n",
            "Epoch 99 | train_loss=455.3008 | val_loss=285.0262\n",
            "Epoch 100 | train_loss=459.7731 | val_loss=290.3390\n",
            "Epoch 101 | train_loss=454.1900 | val_loss=285.4060\n",
            "Epoch 102 | train_loss=456.9347 | val_loss=285.4651\n",
            "Epoch 103 | train_loss=455.9490 | val_loss=285.9626\n",
            "Epoch 104 | train_loss=458.0337 | val_loss=287.0760\n",
            "Epoch 105 | train_loss=457.9106 | val_loss=285.3776\n",
            "Epoch 106 | train_loss=454.8302 | val_loss=286.4984\n",
            "Epoch 107 | train_loss=452.5096 | val_loss=285.1023\n",
            "Epoch 108 | train_loss=455.4400 | val_loss=285.7423\n",
            "Epoch 109 | train_loss=453.6390 | val_loss=290.1077\n",
            "Epoch 110 | train_loss=454.4779 | val_loss=285.8350\n",
            "Epoch 111 | train_loss=459.4934 | val_loss=288.6530\n",
            "Epoch 112 | train_loss=452.8570 | val_loss=286.9397\n",
            "Epoch 113 | train_loss=451.2038 | val_loss=289.2689\n",
            "Epoch 114 | train_loss=456.7939 | val_loss=284.8051\n",
            "Epoch 115 | train_loss=452.5730 | val_loss=289.6661\n",
            "Epoch 116 | train_loss=456.7066 | val_loss=292.7276\n",
            "Epoch 117 | train_loss=453.3295 | val_loss=285.8778\n",
            "Epoch 118 | train_loss=453.7169 | val_loss=286.3935\n",
            "Epoch 119 | train_loss=455.5952 | val_loss=285.1994\n",
            "Epoch 120 | train_loss=452.9791 | val_loss=285.7397\n",
            "Epoch 121 | train_loss=455.6480 | val_loss=285.1444\n",
            "Epoch 122 | train_loss=453.1016 | val_loss=286.6830\n",
            "Epoch 123 | train_loss=453.1783 | val_loss=284.9621\n",
            "Epoch 124 | train_loss=453.7960 | val_loss=288.9210\n",
            "Epoch 125 | train_loss=450.0090 | val_loss=286.6102\n",
            "Epoch 126 | train_loss=453.6880 | val_loss=285.6784\n",
            "Epoch 127 | train_loss=453.2259 | val_loss=284.5086\n",
            "Epoch 128 | train_loss=448.8526 | val_loss=285.4618\n",
            "Epoch 129 | train_loss=450.9944 | val_loss=285.1816\n",
            "Epoch 130 | train_loss=452.6532 | val_loss=286.5043\n",
            "Epoch 131 | train_loss=451.8266 | val_loss=286.3122\n",
            "Epoch 132 | train_loss=452.4123 | val_loss=285.3137\n",
            "Epoch 133 | train_loss=453.6140 | val_loss=286.6793\n",
            "Epoch 134 | train_loss=452.7251 | val_loss=284.5092\n",
            "Epoch 135 | train_loss=449.0042 | val_loss=288.5474\n",
            "Epoch 136 | train_loss=447.4033 | val_loss=297.9723\n",
            "Epoch 137 | train_loss=453.5753 | val_loss=284.9642\n",
            "Epoch 138 | train_loss=451.4193 | val_loss=289.8843\n",
            "Epoch 139 | train_loss=450.0358 | val_loss=284.5830\n",
            "Epoch 140 | train_loss=448.5434 | val_loss=290.6632\n",
            "Epoch 141 | train_loss=452.5439 | val_loss=287.8213\n",
            "Epoch 142 | train_loss=445.1345 | val_loss=290.4800\n",
            "Epoch 143 | train_loss=449.7922 | val_loss=285.6421\n",
            "Epoch 144 | train_loss=450.2636 | val_loss=285.0969\n",
            "Epoch 145 | train_loss=448.2131 | val_loss=286.1475\n",
            "Epoch 146 | train_loss=448.5319 | val_loss=286.0109\n",
            "Epoch 147 | train_loss=450.2256 | val_loss=287.1820\n",
            "Epoch 148 | train_loss=447.4738 | val_loss=286.1991\n",
            "Epoch 149 | train_loss=450.4686 | val_loss=286.1422\n",
            "Epoch 150 | train_loss=450.1108 | val_loss=284.8987\n",
            "Epoch 151 | train_loss=450.1135 | val_loss=285.8298\n",
            "Epoch 152 | train_loss=448.1713 | val_loss=285.1533\n",
            "Epoch 153 | train_loss=449.7560 | val_loss=286.0363\n",
            "Epoch 154 | train_loss=450.5660 | val_loss=293.0808\n",
            "Epoch 155 | train_loss=448.8596 | val_loss=287.9122\n",
            "Epoch 156 | train_loss=448.0881 | val_loss=285.0376\n",
            "Epoch 157 | train_loss=451.5472 | val_loss=287.6767\n",
            "Epoch 158 | train_loss=444.8403 | val_loss=288.9334\n",
            "Epoch 159 | train_loss=447.7312 | val_loss=289.1126\n",
            "Epoch 160 | train_loss=446.3892 | val_loss=285.9339\n",
            "Epoch 161 | train_loss=449.3896 | val_loss=284.3136\n",
            "Epoch 162 | train_loss=446.9157 | val_loss=284.3755\n",
            "Epoch 163 | train_loss=445.4420 | val_loss=284.4588\n",
            "Epoch 164 | train_loss=448.1569 | val_loss=286.4767\n",
            "Epoch 165 | train_loss=446.3637 | val_loss=284.9529\n",
            "Epoch 166 | train_loss=447.2938 | val_loss=286.4189\n",
            "Epoch 167 | train_loss=444.9600 | val_loss=298.9404\n",
            "Epoch 168 | train_loss=445.7735 | val_loss=286.4379\n",
            "Epoch 169 | train_loss=445.3428 | val_loss=287.1434\n",
            "Epoch 170 | train_loss=446.7904 | val_loss=287.0434\n",
            "Epoch 171 | train_loss=449.8116 | val_loss=286.9610\n",
            "Epoch 172 | train_loss=448.0217 | val_loss=284.7158\n",
            "Epoch 173 | train_loss=442.2553 | val_loss=285.0256\n",
            "Epoch 174 | train_loss=445.9635 | val_loss=284.6293\n",
            "Epoch 175 | train_loss=444.0091 | val_loss=289.3370\n",
            "Epoch 176 | train_loss=447.0331 | val_loss=291.5709\n",
            "Epoch 177 | train_loss=446.2248 | val_loss=298.0939\n",
            "Epoch 178 | train_loss=445.4488 | val_loss=299.6089\n",
            "Epoch 179 | train_loss=447.0641 | val_loss=286.4649\n",
            "Epoch 180 | train_loss=440.3572 | val_loss=285.1953\n",
            "Epoch 181 | train_loss=444.2186 | val_loss=289.8385\n",
            "Epoch 182 | train_loss=445.6227 | val_loss=284.8575\n",
            "Epoch 183 | train_loss=444.9335 | val_loss=288.5557\n",
            "Epoch 184 | train_loss=444.9594 | val_loss=285.1331\n",
            "Epoch 185 | train_loss=441.6261 | val_loss=301.1109\n",
            "Epoch 186 | train_loss=445.1019 | val_loss=288.5025\n",
            "Epoch 187 | train_loss=442.2244 | val_loss=289.2344\n",
            "Epoch 188 | train_loss=440.8774 | val_loss=287.8976\n",
            "Epoch 189 | train_loss=441.8045 | val_loss=285.3417\n",
            "Epoch 190 | train_loss=442.8098 | val_loss=285.8001\n",
            "Epoch 191 | train_loss=440.9243 | val_loss=287.3201\n",
            "Epoch 192 | train_loss=441.8699 | val_loss=286.2553\n",
            "Epoch 193 | train_loss=446.5412 | val_loss=284.2164\n",
            "Epoch 194 | train_loss=439.1636 | val_loss=285.9577\n",
            "Epoch 195 | train_loss=446.3864 | val_loss=284.7763\n",
            "Epoch 196 | train_loss=441.7288 | val_loss=291.5279\n",
            "Epoch 197 | train_loss=443.6140 | val_loss=285.9673\n",
            "Epoch 198 | train_loss=440.0887 | val_loss=299.8558\n",
            "Epoch 199 | train_loss=441.3369 | val_loss=287.0716\n",
            "Epoch 200 | train_loss=440.0482 | val_loss=284.9089\n",
            "\n",
            "Evaluating Best Small MLP...\n",
            "Small MLP on VAL  -> RMSE: 16.854, MAE: 2.117\n",
            "Small MLP on TEST -> RMSE: 19.729, MAE: 2.322\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **10. TabNet**\n",
        "\n"
      ],
      "metadata": {
        "id": "mDchISNnTPnK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytorch-tabnet\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "imsIS-cDTWdL",
        "outputId": "4086d47a-c8cb-4fc3-fe8e-0cfd2d7becb6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pytorch-tabnet\n",
            "  Downloading pytorch_tabnet-4.1.0-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from pytorch-tabnet) (2.0.2)\n",
            "Requirement already satisfied: scikit_learn>0.21 in /usr/local/lib/python3.12/dist-packages (from pytorch-tabnet) (1.6.1)\n",
            "Requirement already satisfied: scipy>1.4 in /usr/local/lib/python3.12/dist-packages (from pytorch-tabnet) (1.16.3)\n",
            "Requirement already satisfied: torch>=1.3 in /usr/local/lib/python3.12/dist-packages (from pytorch-tabnet) (2.9.0+cu126)\n",
            "Requirement already satisfied: tqdm>=4.36 in /usr/local/lib/python3.12/dist-packages (from pytorch-tabnet) (4.67.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit_learn>0.21->pytorch-tabnet) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit_learn>0.21->pytorch-tabnet) (3.6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=1.3->pytorch-tabnet) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.3->pytorch-tabnet) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.3->pytorch-tabnet) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.3->pytorch-tabnet) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.3->pytorch-tabnet) (3.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.3->pytorch-tabnet) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.3->pytorch-tabnet) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.3->pytorch-tabnet) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.3->pytorch-tabnet) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.3->pytorch-tabnet) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.3->pytorch-tabnet) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.3->pytorch-tabnet) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.3->pytorch-tabnet) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.3->pytorch-tabnet) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.3->pytorch-tabnet) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.3->pytorch-tabnet) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.3->pytorch-tabnet) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.3->pytorch-tabnet) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=1.3->pytorch-tabnet) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.3->pytorch-tabnet) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.3->pytorch-tabnet) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.3->pytorch-tabnet) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.3->pytorch-tabnet) (3.5.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.3->pytorch-tabnet) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.3->pytorch-tabnet) (3.0.3)\n",
            "Downloading pytorch_tabnet-4.1.0-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.5/44.5 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pytorch-tabnet\n",
            "Successfully installed pytorch-tabnet-4.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pytorch_tabnet.tab_model import TabNetRegressor\n",
        "import numpy as np\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n"
      ],
      "metadata": {
        "id": "txH2E7VDTZ_v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "def preprocess_airbnb(raw_path=RAW_PATH, out_dir=OUT_DIR):\n",
        "    df = pd.read_csv(raw_path)\n",
        "\n",
        "    # Standardize column names: trim spaces, convert to lowercase, replace spaces with underscores\n",
        "    df.columns = [c.strip().lower().replace(\" \", \"_\") for c in df.columns]\n",
        "\n",
        "    # Keep only the columns we need\n",
        "    cols_keep = [\n",
        "        \"price\",\n",
        "        \"service_fee\",\n",
        "        \"room_type\",\n",
        "        \"neighbourhood_group\",\n",
        "        \"neighbourhood\",\n",
        "        \"minimum_nights\",\n",
        "        \"number_of_reviews\",\n",
        "        \"reviews_per_month\",\n",
        "        \"calculated_host_listings_count\",\n",
        "        \"availability_365\",\n",
        "        \"review_rate_number\",\n",
        "        \"lat\",\n",
        "        \"long\",\n",
        "        \"cancellation_policy\",\n",
        "        \"host_identity_verified\",\n",
        "        \"instant_bookable\",\n",
        "        \"construction_year\",\n",
        "        \"country\",\n",
        "        \"country_code\",\n",
        "    ]\n",
        "    cols_keep = [c for c in cols_keep if c in df.columns]\n",
        "    df = df[cols_keep].copy()\n",
        "\n",
        "    target_col = \"price\"\n",
        "\n",
        "    # Clean 'price' and 'service_fee' columns\n",
        "    for col in [target_col, 'service_fee']:\n",
        "        if col in df.columns:\n",
        "            df[col] = df[col].astype(str).str.replace('$', '', regex=False).str.replace(',', '', regex=False)\n",
        "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "\n",
        "    # Basic cleaning: drop rows where price is missing or non-positive\n",
        "    df = df.dropna(subset=[target_col])\n",
        "    df = df[df[target_col] > 0]\n",
        "\n",
        "    numeric_cols = [\n",
        "        \"service_fee\",\n",
        "        \"minimum_nights\",\n",
        "        \"number_of_reviews\",\n",
        "        \"reviews_per_month\",\n",
        "        \"calculated_host_listings_count\",\n",
        "        \"availability_365\",\n",
        "        \"review_rate_number\",\n",
        "        \"lat\",\n",
        "        \"long\",\n",
        "        \"construction_year\",\n",
        "    ]\n",
        "    numeric_cols = [c for c in numeric_cols if c in df.columns]\n",
        "\n",
        "    # Fill NaN values in numeric columns (Median imputation)\n",
        "    for col in numeric_cols:\n",
        "        if df[col].isnull().any():\n",
        "            median_val = df[col].median()\n",
        "            df[col] = df[col].fillna(median_val)\n",
        "            # If median is also NaN (empty column), fill with 0\n",
        "            df[col] = df[col].fillna(0)\n",
        "\n",
        "    categorical_cols = [\n",
        "        \"room_type\",\n",
        "        \"neighbourhood_group\",\n",
        "        \"neighbourhood\",\n",
        "        \"cancellation_policy\",\n",
        "        \"host_identity_verified\",\n",
        "        \"instant_bookable\",\n",
        "        \"country\",\n",
        "        \"country_code\",\n",
        "    ]\n",
        "    categorical_cols = [c for c in categorical_cols if c in df.columns]\n",
        "\n",
        "    # Fill NaN values in categorical columns before encoding\n",
        "    for col in categorical_cols:\n",
        "        if col in df.columns and df[col].isnull().any():\n",
        "            df[col] = df[col].fillna('__MISSING__').astype(str)\n",
        "\n",
        "    # Train/validation/test split\n",
        "    train_df, temp_df = train_test_split(df, test_size=0.2, random_state=42)\n",
        "    val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n",
        "\n",
        "    # Scale numeric features\n",
        "    scaler = StandardScaler()\n",
        "    if numeric_cols:\n",
        "        train_df[numeric_cols] = scaler.fit_transform(train_df[numeric_cols])\n",
        "        val_df[numeric_cols] = scaler.transform(val_df[numeric_cols])\n",
        "        test_df[numeric_cols] = scaler.transform(test_df[numeric_cols])\n",
        "\n",
        "    encoder = OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=-1)\n",
        "\n",
        "    if categorical_cols:\n",
        "        train_cat_encoded = encoder.fit_transform(train_df[categorical_cols])\n",
        "        val_cat_encoded = encoder.transform(val_df[categorical_cols])\n",
        "        test_cat_encoded = encoder.transform(test_df[categorical_cols])\n",
        "\n",
        "        # Map -1 (unknown) to 0, and shift all other categories by 1\n",
        "        train_df[categorical_cols] = np.where(train_cat_encoded == -1, 0, train_cat_encoded + 1).astype(int)\n",
        "        val_df[categorical_cols] = np.where(val_cat_encoded == -1, 0, val_cat_encoded + 1).astype(int)\n",
        "        test_df[categorical_cols] = np.where(test_cat_encoded == -1, 0, test_cat_encoded + 1).astype(int)\n",
        "\n",
        "    train_path = os.path.join(out_dir, \"train_processed.csv\")\n",
        "    val_path = os.path.join(out_dir, \"val_processed.csv\")\n",
        "    test_path = os.path.join(out_dir, \"test_processed.csv\")\n",
        "\n",
        "    train_df.to_csv(train_path, index=False)\n",
        "    val_df.to_csv(val_path, index=False)\n",
        "    test_df.to_csv(test_path, index=False)\n",
        "\n",
        "    metadata = {\n",
        "        \"target_col\": target_col,\n",
        "        \"numeric_cols\": numeric_cols,\n",
        "        \"categorical_cols\": categorical_cols,\n",
        "        \"cat_cardinalities\": {},\n",
        "    }\n",
        "\n",
        "    for i, col in enumerate(categorical_cols):\n",
        "        metadata[\"cat_cardinalities\"][col] = len(encoder.categories_[i]) + 1\n",
        "\n",
        "    meta_path = os.path.join(out_dir, \"metadata.json\")\n",
        "    with open(meta_path, \"w\") as f:\n",
        "        json.dump(metadata, f, indent=2)\n",
        "\n",
        "    print(\"Preprocess done.\")\n",
        "    print(f\"Saved: {train_path}\")\n",
        "    print(f\"Saved: {val_path}\")\n",
        "    print(f\"Saved: {test_path}\")\n",
        "    print(f\"Saved: {meta_path}\")"
      ],
      "metadata": {
        "id": "StN4P6L9T94H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preprocess_airbnb()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZxMapyA8wZWn",
        "outputId": "da3d1344-e518-41fe-cac4-f6e1ad01e841"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2830127989.py:5: DtypeWarning: Columns (25) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df = pd.read_csv(raw_path)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preprocess done.\n",
            "Saved: /content/data/train_processed.csv\n",
            "Saved: /content/data/val_processed.csv\n",
            "Saved: /content/data/test_processed.csv\n",
            "Saved: /content/data/metadata.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from xgboost import XGBRegressor\n",
        "\n",
        "def load_processed_data(data_dir=OUT_DIR):\n",
        "    with open(os.path.join(data_dir, \"metadata.json\"), \"r\") as f:\n",
        "        meta = json.load(f)\n",
        "\n",
        "    target_col = meta[\"target_col\"]\n",
        "    numeric_cols = meta[\"numeric_cols\"]\n",
        "    categorical_cols = meta[\"categorical_cols\"]\n",
        "\n",
        "    train = pd.read_csv(os.path.join(data_dir, \"train_processed.csv\"))\n",
        "    val   = pd.read_csv(os.path.join(data_dir, \"val_processed.csv\"))\n",
        "    test  = pd.read_csv(os.path.join(data_dir, \"test_processed.csv\"))\n",
        "\n",
        "    feature_cols = numeric_cols + categorical_cols\n",
        "\n",
        "    X_train = train[feature_cols].values\n",
        "    y_train = train[target_col].values\n",
        "\n",
        "    X_val = val[feature_cols].values\n",
        "    y_val = val[target_col].values\n",
        "\n",
        "    X_test = test[feature_cols].values\n",
        "    y_test = test[target_col].values\n",
        "\n",
        "    return (X_train, y_train, X_val, y_val, X_test, y_test, meta)"
      ],
      "metadata": {
        "id": "Je4UOopgUJ7t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "load_processed_data(data_dir=OUT_DIR)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4BGv_47vZijZ",
        "outputId": "5860c32a-52d2-444a-cbe7-ee5b54307e94"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[ 1.28327864, -0.18974395, -0.49331112, ...,  2.        ,\n",
              "          1.        ,  1.        ],\n",
              "        [ 0.72465276, -0.12769356,  0.00841602, ...,  1.        ,\n",
              "          1.        ,  1.        ],\n",
              "        [-0.10573705, -0.12769356, -0.41303478, ...,  2.        ,\n",
              "          1.        ,  1.        ],\n",
              "        ...,\n",
              "        [-0.58887294, -0.18974395, -0.53344929, ...,  2.        ,\n",
              "          1.        ,  1.        ],\n",
              "        [ 0.43779083,  0.67896147, -0.19227484, ...,  1.        ,\n",
              "          1.        ,  1.        ],\n",
              "        [-0.25671702, -0.22076914, -0.43310387, ...,  2.        ,\n",
              "          1.        ,  1.        ]]),\n",
              " array([1050.,  867.,  590., ...,  432.,  772.,  540.]),\n",
              " array([[ 1.49465059, -0.18974395, -0.53344929, ...,  2.        ,\n",
              "          1.        ,  1.        ],\n",
              "        [ 0.13583089, -0.15871875,  0.55028134, ...,  1.        ,\n",
              "          1.        ,  1.        ],\n",
              "        [-1.31357678, -0.18974395,  0.34959048, ...,  1.        ,\n",
              "          1.        ,  1.        ],\n",
              "        ...,\n",
              "        [ 1.49465059, -0.18974395, -0.55351838, ...,  2.        ,\n",
              "          1.        ,  1.        ],\n",
              "        [ 0.84543673, -0.22076914,  0.57035042, ...,  2.        ,\n",
              "          1.        ,  1.        ],\n",
              "        [-0.45299097, -0.15871875,  0.14889962, ...,  1.        ,\n",
              "          1.        ,  1.        ]]),\n",
              " array([1120.,  672.,  191., ..., 1118.,  903.,  476.]),\n",
              " array([[-0.73985291, -0.18974395, -0.19227484, ...,  2.        ,\n",
              "          1.        ,  1.        ],\n",
              "        [-0.98142085, -0.15871875, -0.51338021, ...,  2.        ,\n",
              "          1.        ,  1.        ],\n",
              "        [-0.95122486, -0.22076914, -0.53344929, ...,  2.        ,\n",
              "          1.        ,  1.        ],\n",
              "        ...,\n",
              "        [-0.40769698, -0.22076914, -0.45317295, ...,  1.        ,\n",
              "          1.        ,  1.        ],\n",
              "        [ 1.52484658, -0.15871875, -0.31268935, ...,  1.        ,\n",
              "          1.        ,  1.        ],\n",
              "        [-0.87573488, -0.03461798,  0.04855419, ...,  2.        ,\n",
              "          1.        ,  1.        ]]),\n",
              " array([ 380.,  300.,  309., ...,  492., 1132.,  336.]),\n",
              " {'target_col': 'price',\n",
              "  'numeric_cols': ['service_fee',\n",
              "   'minimum_nights',\n",
              "   'number_of_reviews',\n",
              "   'reviews_per_month',\n",
              "   'calculated_host_listings_count',\n",
              "   'availability_365',\n",
              "   'review_rate_number',\n",
              "   'lat',\n",
              "   'long',\n",
              "   'construction_year'],\n",
              "  'categorical_cols': ['room_type',\n",
              "   'neighbourhood_group',\n",
              "   'neighbourhood',\n",
              "   'cancellation_policy',\n",
              "   'host_identity_verified',\n",
              "   'instant_bookable',\n",
              "   'country',\n",
              "   'country_code'],\n",
              "  'cat_cardinalities': {'room_type': 5,\n",
              "   'neighbourhood_group': 9,\n",
              "   'neighbourhood': 226,\n",
              "   'cancellation_policy': 5,\n",
              "   'host_identity_verified': 4,\n",
              "   'instant_bookable': 4,\n",
              "   'country': 3,\n",
              "   'country_code': 3}})"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_tabnet(\n",
        "    X_train, y_train,\n",
        "    X_val, y_val,\n",
        "    X_test, y_test\n",
        "):\n",
        "\n",
        "    tabnet = TabNetRegressor(\n",
        "        n_d=16,\n",
        "        n_a=16,\n",
        "        n_steps=5,\n",
        "        gamma=1.5,\n",
        "        lambda_sparse=1e-4,\n",
        "        optimizer_fn=torch.optim.Adam,\n",
        "        optimizer_params=dict(lr=1e-3),\n",
        "        mask_type='entmax',\n",
        "        seed=42\n",
        "    )\n",
        "\n",
        "    tabnet.fit(\n",
        "        X_train, y_train,\n",
        "        eval_set=[(X_val, y_val)],\n",
        "        eval_metric=['rmse'],\n",
        "        max_epochs=400,\n",
        "        patience=20,\n",
        "        batch_size=8192,\n",
        "        virtual_batch_size=256,\n",
        "        num_workers=0,\n",
        "        drop_last=False\n",
        "    )\n",
        "\n",
        "    # ----------- Evaluation -------------\n",
        "    pred_val = tabnet.predict(X_val)\n",
        "    pred_test = tabnet.predict(X_test)\n",
        "\n",
        "    # RMSE / MAE\n",
        "    rmse_val = mean_squared_error(y_val, pred_val) ** 0.5\n",
        "    mae_val = mean_absolute_error(y_val, pred_val)\n",
        "\n",
        "    rmse_test = mean_squared_error(y_test, pred_test) ** 0.5\n",
        "    mae_test = mean_absolute_error(y_test, pred_test)\n",
        "\n",
        "    print(f\"TabNet -> VAL RMSE: {rmse_val:.3f}, MAE: {mae_val:.3f}\")\n",
        "    print(f\"TabNet -> TEST RMSE: {rmse_test:.3f}, MAE: {mae_test:.3f}\")\n",
        "\n",
        "    return tabnet\n"
      ],
      "metadata": {
        "id": "Lha4FK5WTbhJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, y_train, X_val, y_val, X_test, y_test, meta = load_processed_data()\n",
        "y_train = y_train.reshape(-1, 1)\n",
        "y_val = y_val.reshape(-1, 1)\n",
        "y_test = y_test.reshape(-1, 1)\n",
        "tabnet_model = train_tabnet(\n",
        "    X_train, y_train,\n",
        "    X_val, y_val,\n",
        "    X_test, y_test\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HSk7wpa7TeHC",
        "outputId": "11f7b5db-7769-4383-e036-38409931e044"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cuda\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0  | loss: 501745.34362| val_0_rmse: 706.71718|  0:00:02s\n",
            "epoch 1  | loss: 500999.21397| val_0_rmse: 705.69761|  0:00:04s\n",
            "epoch 2  | loss: 500248.8425| val_0_rmse: 705.28109|  0:00:06s\n",
            "epoch 3  | loss: 499535.77327| val_0_rmse: 704.49901|  0:00:08s\n",
            "epoch 4  | loss: 498839.67627| val_0_rmse: 704.42462|  0:00:10s\n",
            "epoch 5  | loss: 498190.80584| val_0_rmse: 704.12135|  0:00:13s\n",
            "epoch 6  | loss: 497536.61943| val_0_rmse: 703.72471|  0:00:15s\n",
            "epoch 7  | loss: 496812.12182| val_0_rmse: 703.5244|  0:00:17s\n",
            "epoch 8  | loss: 496068.94681| val_0_rmse: 702.96108|  0:00:19s\n",
            "epoch 9  | loss: 495350.19775| val_0_rmse: 702.91201|  0:00:21s\n",
            "epoch 10 | loss: 494511.31647| val_0_rmse: 702.31932|  0:00:24s\n",
            "epoch 11 | loss: 493711.88155| val_0_rmse: 701.73068|  0:00:26s\n",
            "epoch 12 | loss: 492752.4071| val_0_rmse: 700.94928|  0:00:28s\n",
            "epoch 13 | loss: 491719.74154| val_0_rmse: 700.24784|  0:00:30s\n",
            "epoch 14 | loss: 490560.99944| val_0_rmse: 699.51478|  0:00:32s\n",
            "epoch 15 | loss: 489248.86537| val_0_rmse: 698.43178|  0:00:34s\n",
            "epoch 16 | loss: 487770.07644| val_0_rmse: 697.4475|  0:00:37s\n",
            "epoch 17 | loss: 486217.59731| val_0_rmse: 696.42026|  0:00:39s\n",
            "epoch 18 | loss: 484627.13196| val_0_rmse: 695.29866|  0:00:41s\n",
            "epoch 19 | loss: 483059.67641| val_0_rmse: 693.97371|  0:00:43s\n",
            "epoch 20 | loss: 481463.07842| val_0_rmse: 692.90081|  0:00:45s\n",
            "epoch 21 | loss: 479755.30556| val_0_rmse: 691.46067|  0:00:48s\n",
            "epoch 22 | loss: 478112.36624| val_0_rmse: 690.1982|  0:00:50s\n",
            "epoch 23 | loss: 476336.13358| val_0_rmse: 688.75811|  0:00:52s\n",
            "epoch 24 | loss: 474435.52479| val_0_rmse: 687.24791|  0:00:54s\n",
            "epoch 25 | loss: 472559.13856| val_0_rmse: 686.5443|  0:00:56s\n",
            "epoch 26 | loss: 470743.48815| val_0_rmse: 685.38728|  0:00:59s\n",
            "epoch 27 | loss: 468922.74988| val_0_rmse: 683.12576|  0:01:01s\n",
            "epoch 28 | loss: 466975.71223| val_0_rmse: 681.44464|  0:01:03s\n",
            "epoch 29 | loss: 464982.66443| val_0_rmse: 680.107 |  0:01:05s\n",
            "epoch 30 | loss: 462806.60449| val_0_rmse: 678.59786|  0:01:07s\n",
            "epoch 31 | loss: 460647.8085| val_0_rmse: 676.80813|  0:01:09s\n",
            "epoch 32 | loss: 458629.85396| val_0_rmse: 675.24616|  0:01:12s\n",
            "epoch 33 | loss: 456509.1298| val_0_rmse: 673.73303|  0:01:14s\n",
            "epoch 34 | loss: 454274.52422| val_0_rmse: 672.37144|  0:01:16s\n",
            "epoch 35 | loss: 452010.17299| val_0_rmse: 670.27445|  0:01:18s\n",
            "epoch 36 | loss: 449652.03105| val_0_rmse: 668.66246|  0:01:20s\n",
            "epoch 37 | loss: 447083.85251| val_0_rmse: 666.52253|  0:01:23s\n",
            "epoch 38 | loss: 444437.61997| val_0_rmse: 664.22535|  0:01:25s\n",
            "epoch 39 | loss: 441846.12489| val_0_rmse: 662.36378|  0:01:27s\n",
            "epoch 40 | loss: 439278.07079| val_0_rmse: 660.6786|  0:01:29s\n",
            "epoch 41 | loss: 436500.63535| val_0_rmse: 658.11882|  0:01:31s\n",
            "epoch 42 | loss: 433617.24214| val_0_rmse: 655.88256|  0:01:33s\n",
            "epoch 43 | loss: 430703.08234| val_0_rmse: 653.40811|  0:01:36s\n",
            "epoch 44 | loss: 427746.92745| val_0_rmse: 650.90706|  0:01:38s\n",
            "epoch 45 | loss: 424783.78339| val_0_rmse: 649.06868|  0:01:40s\n",
            "epoch 46 | loss: 421823.08201| val_0_rmse: 646.66105|  0:01:42s\n",
            "epoch 47 | loss: 418921.17279| val_0_rmse: 644.47004|  0:01:44s\n",
            "epoch 48 | loss: 415888.24034| val_0_rmse: 642.63932|  0:01:47s\n",
            "epoch 49 | loss: 412910.33692| val_0_rmse: 640.38455|  0:01:49s\n",
            "epoch 50 | loss: 410016.11714| val_0_rmse: 637.37786|  0:01:51s\n",
            "epoch 51 | loss: 406943.37603| val_0_rmse: 634.76303|  0:01:53s\n",
            "epoch 52 | loss: 403662.35311| val_0_rmse: 632.12635|  0:01:56s\n",
            "epoch 53 | loss: 400467.70509| val_0_rmse: 629.79443|  0:01:58s\n",
            "epoch 54 | loss: 397158.3068| val_0_rmse: 627.69036|  0:02:00s\n",
            "epoch 55 | loss: 393782.17734| val_0_rmse: 624.60261|  0:02:02s\n",
            "epoch 56 | loss: 390271.28726| val_0_rmse: 621.69635|  0:02:04s\n",
            "epoch 57 | loss: 386906.44735| val_0_rmse: 619.31267|  0:02:06s\n",
            "epoch 58 | loss: 383590.00814| val_0_rmse: 616.91388|  0:02:09s\n",
            "epoch 59 | loss: 380115.73787| val_0_rmse: 613.51985|  0:02:11s\n",
            "epoch 60 | loss: 376428.84722| val_0_rmse: 610.75208|  0:02:14s\n",
            "epoch 61 | loss: 372634.2772| val_0_rmse: 607.08363|  0:02:16s\n",
            "epoch 62 | loss: 368842.01096| val_0_rmse: 604.30164|  0:02:18s\n",
            "epoch 63 | loss: 365126.66187| val_0_rmse: 601.77189|  0:02:20s\n",
            "epoch 64 | loss: 361532.51667| val_0_rmse: 598.36728|  0:02:22s\n",
            "epoch 65 | loss: 357708.66204| val_0_rmse: 594.74363|  0:02:25s\n",
            "epoch 66 | loss: 353582.51035| val_0_rmse: 590.94865|  0:02:27s\n",
            "epoch 67 | loss: 349483.89881| val_0_rmse: 587.94459|  0:02:29s\n",
            "epoch 68 | loss: 345356.06275| val_0_rmse: 585.01511|  0:02:31s\n",
            "epoch 69 | loss: 341218.6988| val_0_rmse: 579.99607|  0:02:33s\n",
            "epoch 70 | loss: 337084.99673| val_0_rmse: 576.73357|  0:02:35s\n",
            "epoch 71 | loss: 332786.26198| val_0_rmse: 573.61941|  0:02:38s\n",
            "epoch 72 | loss: 328307.42633| val_0_rmse: 569.19702|  0:02:40s\n",
            "epoch 73 | loss: 323753.83133| val_0_rmse: 567.45879|  0:02:42s\n",
            "epoch 74 | loss: 319806.32028| val_0_rmse: 562.82871|  0:02:44s\n",
            "epoch 75 | loss: 315774.3299| val_0_rmse: 559.37071|  0:02:46s\n",
            "epoch 76 | loss: 311673.56964| val_0_rmse: 555.89656|  0:02:49s\n",
            "epoch 77 | loss: 307509.37021| val_0_rmse: 551.04553|  0:02:51s\n",
            "epoch 78 | loss: 303306.33123| val_0_rmse: 548.08822|  0:02:53s\n",
            "epoch 79 | loss: 299108.35115| val_0_rmse: 543.7683|  0:02:55s\n",
            "epoch 80 | loss: 295021.9876| val_0_rmse: 540.21508|  0:02:57s\n",
            "epoch 81 | loss: 290951.61536| val_0_rmse: 535.79426|  0:03:00s\n",
            "epoch 82 | loss: 286903.65329| val_0_rmse: 532.43858|  0:03:02s\n",
            "epoch 83 | loss: 282814.63763| val_0_rmse: 527.89836|  0:03:04s\n",
            "epoch 84 | loss: 278656.91335| val_0_rmse: 525.50662|  0:03:06s\n",
            "epoch 85 | loss: 274457.69612| val_0_rmse: 521.37435|  0:03:08s\n",
            "epoch 86 | loss: 270218.66534| val_0_rmse: 516.98783|  0:03:11s\n",
            "epoch 87 | loss: 265966.72849| val_0_rmse: 513.99907|  0:03:13s\n",
            "epoch 88 | loss: 261681.58183| val_0_rmse: 507.63337|  0:03:15s\n",
            "epoch 89 | loss: 257476.25889| val_0_rmse: 503.6757|  0:03:17s\n",
            "epoch 90 | loss: 253229.10271| val_0_rmse: 500.01116|  0:03:20s\n",
            "epoch 91 | loss: 249092.00748| val_0_rmse: 495.7103|  0:03:22s\n",
            "epoch 92 | loss: 244859.00256| val_0_rmse: 490.08142|  0:03:24s\n",
            "epoch 93 | loss: 240610.78516| val_0_rmse: 489.0141|  0:03:27s\n",
            "epoch 94 | loss: 236446.87764| val_0_rmse: 482.97114|  0:03:29s\n",
            "epoch 95 | loss: 232204.53757| val_0_rmse: 478.22122|  0:03:31s\n",
            "epoch 96 | loss: 227896.17638| val_0_rmse: 473.01978|  0:03:33s\n",
            "epoch 97 | loss: 223533.55048| val_0_rmse: 470.48941|  0:03:35s\n",
            "epoch 98 | loss: 219305.53381| val_0_rmse: 462.21152|  0:03:38s\n",
            "epoch 99 | loss: 215173.44044| val_0_rmse: 461.00894|  0:03:40s\n",
            "epoch 100| loss: 210947.63459| val_0_rmse: 453.39232|  0:03:42s\n",
            "epoch 101| loss: 206762.67508| val_0_rmse: 453.79418|  0:03:44s\n",
            "epoch 102| loss: 202652.89783| val_0_rmse: 443.66898|  0:03:46s\n",
            "epoch 103| loss: 198629.62362| val_0_rmse: 442.79305|  0:03:49s\n",
            "epoch 104| loss: 194755.16995| val_0_rmse: 439.16339|  0:03:51s\n",
            "epoch 105| loss: 190635.6567| val_0_rmse: 431.77277|  0:03:53s\n",
            "epoch 106| loss: 186753.4791| val_0_rmse: 427.63491|  0:03:55s\n",
            "epoch 107| loss: 182808.88894| val_0_rmse: 424.69184|  0:03:58s\n",
            "epoch 108| loss: 178983.13853| val_0_rmse: 419.59461|  0:04:00s\n",
            "epoch 109| loss: 174953.3638| val_0_rmse: 416.6006|  0:04:02s\n",
            "epoch 110| loss: 171018.37923| val_0_rmse: 409.26049|  0:04:05s\n",
            "epoch 111| loss: 167060.411| val_0_rmse: 403.00976|  0:04:06s\n",
            "epoch 112| loss: 163250.25784| val_0_rmse: 400.76487|  0:04:09s\n",
            "epoch 113| loss: 159430.17048| val_0_rmse: 395.36073|  0:04:11s\n",
            "epoch 114| loss: 155694.10896| val_0_rmse: 388.83025|  0:04:14s\n",
            "epoch 115| loss: 151825.54167| val_0_rmse: 384.1139|  0:04:16s\n",
            "epoch 116| loss: 148105.57698| val_0_rmse: 380.62492|  0:04:18s\n",
            "epoch 117| loss: 144295.82158| val_0_rmse: 376.27385|  0:04:20s\n",
            "epoch 118| loss: 140566.46362| val_0_rmse: 368.22584|  0:04:22s\n",
            "epoch 119| loss: 136715.17438| val_0_rmse: 363.03333|  0:04:24s\n",
            "epoch 120| loss: 133092.38514| val_0_rmse: 358.27333|  0:04:27s\n",
            "epoch 121| loss: 129379.56359| val_0_rmse: 356.5638|  0:04:29s\n",
            "epoch 122| loss: 125677.33684| val_0_rmse: 348.46795|  0:04:31s\n",
            "epoch 123| loss: 122129.5469| val_0_rmse: 341.89576|  0:04:33s\n",
            "epoch 124| loss: 118606.2638| val_0_rmse: 339.25833|  0:04:35s\n",
            "epoch 125| loss: 115154.11065| val_0_rmse: 334.26832|  0:04:38s\n",
            "epoch 126| loss: 111710.19221| val_0_rmse: 331.43073|  0:04:40s\n",
            "epoch 127| loss: 108278.32159| val_0_rmse: 326.13161|  0:04:42s\n",
            "epoch 128| loss: 104787.51663| val_0_rmse: 321.05519|  0:04:44s\n",
            "epoch 129| loss: 101431.904| val_0_rmse: 316.58371|  0:04:46s\n",
            "epoch 130| loss: 98070.39161| val_0_rmse: 308.05909|  0:04:49s\n",
            "epoch 131| loss: 94687.54839| val_0_rmse: 303.18983|  0:04:51s\n",
            "epoch 132| loss: 91389.68478| val_0_rmse: 296.68581|  0:04:53s\n",
            "epoch 133| loss: 88168.1076| val_0_rmse: 291.26511|  0:04:56s\n",
            "epoch 134| loss: 85143.80613| val_0_rmse: 286.95779|  0:04:57s\n",
            "epoch 135| loss: 82044.91599| val_0_rmse: 280.53413|  0:05:00s\n",
            "epoch 136| loss: 78995.36622| val_0_rmse: 276.50153|  0:05:02s\n",
            "epoch 137| loss: 76137.60028| val_0_rmse: 268.9817|  0:05:04s\n",
            "epoch 138| loss: 73361.16632| val_0_rmse: 267.87737|  0:05:06s\n",
            "epoch 139| loss: 70423.65462| val_0_rmse: 263.7902|  0:05:09s\n",
            "epoch 140| loss: 67652.91215| val_0_rmse: 259.85288|  0:05:11s\n",
            "epoch 141| loss: 64821.83322| val_0_rmse: 252.28727|  0:05:13s\n",
            "epoch 142| loss: 62024.2736| val_0_rmse: 239.55562|  0:05:16s\n",
            "epoch 143| loss: 59325.82484| val_0_rmse: 235.37775|  0:05:18s\n",
            "epoch 144| loss: 56578.87335| val_0_rmse: 235.82503|  0:05:20s\n",
            "epoch 145| loss: 53999.7914| val_0_rmse: 231.58208|  0:05:22s\n",
            "epoch 146| loss: 51523.97018| val_0_rmse: 222.52085|  0:05:24s\n",
            "epoch 147| loss: 48938.12166| val_0_rmse: 221.60528|  0:05:27s\n",
            "epoch 148| loss: 46586.19337| val_0_rmse: 219.36181|  0:05:29s\n",
            "epoch 149| loss: 44154.84427| val_0_rmse: 209.30388|  0:05:31s\n",
            "epoch 150| loss: 41731.82567| val_0_rmse: 201.85305|  0:05:33s\n",
            "epoch 151| loss: 39501.05008| val_0_rmse: 195.19203|  0:05:35s\n",
            "epoch 152| loss: 37251.49895| val_0_rmse: 187.32564|  0:05:38s\n",
            "epoch 153| loss: 35076.80609| val_0_rmse: 178.85193|  0:05:40s\n",
            "epoch 154| loss: 32900.26783| val_0_rmse: 174.86013|  0:05:42s\n",
            "epoch 155| loss: 30935.99796| val_0_rmse: 167.29211|  0:05:44s\n",
            "epoch 156| loss: 28926.01791| val_0_rmse: 164.76913|  0:05:46s\n",
            "epoch 157| loss: 27077.61194| val_0_rmse: 166.73268|  0:05:49s\n",
            "epoch 158| loss: 25094.13013| val_0_rmse: 155.80265|  0:05:51s\n",
            "epoch 159| loss: 23388.26754| val_0_rmse: 147.27006|  0:05:53s\n",
            "epoch 160| loss: 21581.08896| val_0_rmse: 142.21668|  0:05:55s\n",
            "epoch 161| loss: 19959.19523| val_0_rmse: 136.91835|  0:05:57s\n",
            "epoch 162| loss: 18393.46131| val_0_rmse: 128.25993|  0:05:59s\n",
            "epoch 163| loss: 16916.59098| val_0_rmse: 127.72382|  0:06:02s\n",
            "epoch 164| loss: 15509.73652| val_0_rmse: 114.7281|  0:06:04s\n",
            "epoch 165| loss: 14128.50733| val_0_rmse: 109.26531|  0:06:06s\n",
            "epoch 166| loss: 12908.59371| val_0_rmse: 114.81923|  0:06:08s\n",
            "epoch 167| loss: 11591.8167| val_0_rmse: 109.20272|  0:06:10s\n",
            "epoch 168| loss: 10426.45902| val_0_rmse: 98.85874|  0:06:13s\n",
            "epoch 169| loss: 9419.93047| val_0_rmse: 95.4772 |  0:06:15s\n",
            "epoch 170| loss: 8345.21844| val_0_rmse: 88.73921|  0:06:18s\n",
            "epoch 171| loss: 7457.69424| val_0_rmse: 73.90307|  0:06:19s\n",
            "epoch 172| loss: 6488.50379| val_0_rmse: 78.15943|  0:06:22s\n",
            "epoch 173| loss: 5723.87229| val_0_rmse: 74.57864|  0:06:24s\n",
            "epoch 174| loss: 5018.93499| val_0_rmse: 71.68166|  0:06:26s\n",
            "epoch 175| loss: 4277.92732| val_0_rmse: 61.46799|  0:06:28s\n",
            "epoch 176| loss: 3703.70603| val_0_rmse: 51.83186|  0:06:30s\n",
            "epoch 177| loss: 3197.45601| val_0_rmse: 46.89566|  0:06:32s\n",
            "epoch 178| loss: 2642.87393| val_0_rmse: 54.25924|  0:06:35s\n",
            "epoch 179| loss: 2203.49085| val_0_rmse: 47.07795|  0:06:37s\n",
            "epoch 180| loss: 1869.91855| val_0_rmse: 41.76075|  0:06:39s\n",
            "epoch 181| loss: 1572.11838| val_0_rmse: 44.89913|  0:06:41s\n",
            "epoch 182| loss: 1398.41119| val_0_rmse: 27.09041|  0:06:43s\n",
            "epoch 183| loss: 1240.25986| val_0_rmse: 33.58169|  0:06:46s\n",
            "epoch 184| loss: 1070.59084| val_0_rmse: 34.08231|  0:06:48s\n",
            "epoch 185| loss: 957.27402| val_0_rmse: 24.0848 |  0:06:50s\n",
            "epoch 186| loss: 902.57883| val_0_rmse: 22.85619|  0:06:52s\n",
            "epoch 187| loss: 923.00314| val_0_rmse: 27.78617|  0:06:54s\n",
            "epoch 188| loss: 783.91446| val_0_rmse: 20.23513|  0:06:56s\n",
            "epoch 189| loss: 831.57838| val_0_rmse: 21.43469|  0:06:58s\n",
            "epoch 190| loss: 837.70356| val_0_rmse: 23.26311|  0:07:00s\n",
            "epoch 191| loss: 809.67361| val_0_rmse: 21.7583 |  0:07:03s\n",
            "epoch 192| loss: 826.06434| val_0_rmse: 28.08597|  0:07:05s\n",
            "epoch 193| loss: 766.87582| val_0_rmse: 21.02659|  0:07:07s\n",
            "epoch 194| loss: 693.53531| val_0_rmse: 19.51817|  0:07:09s\n",
            "epoch 195| loss: 660.73606| val_0_rmse: 19.28613|  0:07:11s\n",
            "epoch 196| loss: 671.15932| val_0_rmse: 20.02702|  0:07:14s\n",
            "epoch 197| loss: 634.37589| val_0_rmse: 26.1917 |  0:07:16s\n",
            "epoch 198| loss: 647.17478| val_0_rmse: 21.35775|  0:07:18s\n",
            "epoch 199| loss: 565.45101| val_0_rmse: 19.45055|  0:07:20s\n",
            "epoch 200| loss: 590.58969| val_0_rmse: 20.20631|  0:07:22s\n",
            "epoch 201| loss: 580.63491| val_0_rmse: 19.11518|  0:07:25s\n",
            "epoch 202| loss: 534.4609| val_0_rmse: 18.69375|  0:07:27s\n",
            "epoch 203| loss: 538.13634| val_0_rmse: 19.36723|  0:07:29s\n",
            "epoch 204| loss: 577.32342| val_0_rmse: 18.80437|  0:07:31s\n",
            "epoch 205| loss: 486.80635| val_0_rmse: 18.57575|  0:07:34s\n",
            "epoch 206| loss: 520.35698| val_0_rmse: 18.40177|  0:07:36s\n",
            "epoch 207| loss: 511.43926| val_0_rmse: 20.06463|  0:07:38s\n",
            "epoch 208| loss: 506.37381| val_0_rmse: 18.09221|  0:07:40s\n",
            "epoch 209| loss: 486.11285| val_0_rmse: 18.41957|  0:07:42s\n",
            "epoch 210| loss: 471.41425| val_0_rmse: 18.42165|  0:07:44s\n",
            "epoch 211| loss: 472.87197| val_0_rmse: 19.46996|  0:07:47s\n",
            "epoch 212| loss: 462.22075| val_0_rmse: 18.5164 |  0:07:49s\n",
            "epoch 213| loss: 456.76716| val_0_rmse: 21.34555|  0:07:51s\n",
            "epoch 214| loss: 461.63077| val_0_rmse: 17.88383|  0:07:53s\n",
            "epoch 215| loss: 458.49636| val_0_rmse: 17.79076|  0:07:55s\n",
            "epoch 216| loss: 429.85374| val_0_rmse: 17.94742|  0:07:58s\n",
            "epoch 217| loss: 440.24412| val_0_rmse: 19.31342|  0:08:00s\n",
            "epoch 218| loss: 468.32511| val_0_rmse: 18.50366|  0:08:02s\n",
            "epoch 219| loss: 421.40073| val_0_rmse: 17.84082|  0:08:04s\n",
            "epoch 220| loss: 450.88731| val_0_rmse: 18.57059|  0:08:06s\n",
            "epoch 221| loss: 471.06398| val_0_rmse: 18.73335|  0:08:08s\n",
            "epoch 222| loss: 434.83137| val_0_rmse: 20.2021 |  0:08:11s\n",
            "epoch 223| loss: 418.18619| val_0_rmse: 17.79076|  0:08:13s\n",
            "epoch 224| loss: 428.25563| val_0_rmse: 18.04816|  0:08:15s\n",
            "epoch 225| loss: 407.09305| val_0_rmse: 18.84121|  0:08:17s\n",
            "epoch 226| loss: 423.98351| val_0_rmse: 19.27075|  0:08:19s\n",
            "epoch 227| loss: 396.11055| val_0_rmse: 17.82195|  0:08:22s\n",
            "epoch 228| loss: 418.63527| val_0_rmse: 18.1722 |  0:08:24s\n",
            "epoch 229| loss: 397.64154| val_0_rmse: 19.06462|  0:08:26s\n",
            "epoch 230| loss: 409.13639| val_0_rmse: 19.1294 |  0:08:29s\n",
            "epoch 231| loss: 405.66476| val_0_rmse: 17.5146 |  0:08:30s\n",
            "epoch 232| loss: 403.52263| val_0_rmse: 21.05251|  0:08:32s\n",
            "epoch 233| loss: 402.76569| val_0_rmse: 18.25007|  0:08:35s\n",
            "epoch 234| loss: 384.85911| val_0_rmse: 21.13806|  0:08:37s\n",
            "epoch 235| loss: 399.68378| val_0_rmse: 18.27703|  0:08:40s\n",
            "epoch 236| loss: 391.92257| val_0_rmse: 17.72833|  0:08:42s\n",
            "epoch 237| loss: 388.64312| val_0_rmse: 20.21016|  0:08:44s\n",
            "epoch 238| loss: 388.45699| val_0_rmse: 17.5979 |  0:08:46s\n",
            "epoch 239| loss: 397.00465| val_0_rmse: 19.21392|  0:08:48s\n",
            "epoch 240| loss: 378.23171| val_0_rmse: 21.00021|  0:08:50s\n",
            "epoch 241| loss: 404.69216| val_0_rmse: 18.67169|  0:08:53s\n",
            "epoch 242| loss: 398.1486| val_0_rmse: 17.50344|  0:08:55s\n",
            "epoch 243| loss: 383.49821| val_0_rmse: 17.75077|  0:08:57s\n",
            "epoch 244| loss: 376.3081| val_0_rmse: 17.63104|  0:08:59s\n",
            "epoch 245| loss: 373.46861| val_0_rmse: 20.45153|  0:09:01s\n",
            "epoch 246| loss: 369.26209| val_0_rmse: 17.84938|  0:09:04s\n",
            "epoch 247| loss: 362.61777| val_0_rmse: 17.95639|  0:09:06s\n",
            "epoch 248| loss: 379.16114| val_0_rmse: 18.24319|  0:09:08s\n",
            "epoch 249| loss: 383.61465| val_0_rmse: 17.26034|  0:09:10s\n",
            "epoch 250| loss: 367.36353| val_0_rmse: 17.48532|  0:09:12s\n",
            "epoch 251| loss: 373.52205| val_0_rmse: 17.9411 |  0:09:14s\n",
            "epoch 252| loss: 365.67062| val_0_rmse: 18.29829|  0:09:17s\n",
            "epoch 253| loss: 372.38269| val_0_rmse: 17.90713|  0:09:19s\n",
            "epoch 254| loss: 380.54054| val_0_rmse: 19.2679 |  0:09:21s\n",
            "epoch 255| loss: 373.6308| val_0_rmse: 17.42024|  0:09:23s\n",
            "epoch 256| loss: 359.54692| val_0_rmse: 17.55571|  0:09:25s\n",
            "epoch 257| loss: 373.70293| val_0_rmse: 17.94353|  0:09:28s\n",
            "epoch 258| loss: 365.1929| val_0_rmse: 17.37317|  0:09:30s\n",
            "epoch 259| loss: 365.99214| val_0_rmse: 17.75607|  0:09:32s\n",
            "epoch 260| loss: 356.84849| val_0_rmse: 18.89369|  0:09:34s\n",
            "epoch 261| loss: 357.06274| val_0_rmse: 17.84025|  0:09:36s\n",
            "epoch 262| loss: 344.53467| val_0_rmse: 17.87579|  0:09:39s\n",
            "epoch 263| loss: 359.76197| val_0_rmse: 17.46496|  0:09:41s\n",
            "epoch 264| loss: 359.86929| val_0_rmse: 18.22495|  0:09:43s\n",
            "epoch 265| loss: 373.00533| val_0_rmse: 17.26899|  0:09:45s\n",
            "epoch 266| loss: 403.94672| val_0_rmse: 17.5896 |  0:09:47s\n",
            "epoch 267| loss: 360.99749| val_0_rmse: 17.5526 |  0:09:49s\n",
            "epoch 268| loss: 352.27998| val_0_rmse: 17.32951|  0:09:52s\n",
            "epoch 269| loss: 371.12565| val_0_rmse: 17.79556|  0:09:54s\n",
            "\n",
            "Early stopping occurred at epoch 269 with best_epoch = 249 and best_val_0_rmse = 17.26034\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TabNet -> VAL RMSE: 17.260, MAE: 3.348\n",
            "TabNet -> TEST RMSE: 20.055, MAE: 3.576\n"
          ]
        }
      ]
    }
  ]
}